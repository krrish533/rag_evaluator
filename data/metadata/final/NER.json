{
  "filename": "NER.txt",
  "context": "Citation: Park, Y.; Son, G.; Rho, M.\nBiomedical Flat and Nested Named\nEntity Recognition: Methods,\nChallenges, and Advances. Appl. Sci.\n2024 ,14, 9302. https://doi.org/\n10.3390/app14209302\nAcademic Editors: Lykourgos\nMagafas and Rui Ara \u00fajo\nReceived: 20 August 2024\nRevised: 3 October 2024\nAccepted: 5 October 2024\nPublished: 12 October 2024\nCopyright: \u00a92024 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\napplied  \nsciences \nReview\nBiomedical Flat and Nested Named Entity Recognition: Methods,\nChallenges, and Advances\nYesol Park1\n, Gyujin Son2and Mina Rho1,2,3,*\n1Department of Computer Science, Hanyang University, Seoul 04763, Republic of Korea;\nyesolpark@hanyang.ac.kr\n2Department of Artificial Intelligence, Hanyang University, Seoul 04763, Republic of Korea;\ncnbe5494@hanyang.ac.kr\n3Department of Biomedical Informatics, Hanyang University, Seoul 04763, Republic of Korea\n*Correspondence: minarho@hanyang.ac.kr\nAbstract: Biomedical named entity recognition (BioNER) aims to identify and classify biomedical\nentities (i.e., diseases, chemicals, and genes) from text into predefined classes. This process serves as\nan important initial step in extracting biomedical information from textual sources. Considering the\nstructure of the entities it addresses, BioNER tasks are divided into two categories: flat NER, where\nentities are non-overlapping, and nested NER, which identifies entities embedded within another.\nWhile early studies primarily addressed flat NER, recent advances in neural models have enabled\nmore sophisticated approaches to nested NER, gaining increasing relevance in the biomedical field,\nwhere entity relationships are often complex and hierarchically structured. This review, thus, focuses\non the latest progress in large-scale pre-trained language model-based approaches, which have shown\nthe significantly improved performance of NER. The state-of-the-art flat NER models have achieved\naverage F1-scores of 84% on BC2GM, 89% on NCBI Disease, and 92% on BC4CHEM, while nested NER\nmodels have reached 80% on the GENIA dataset, indicating room for enhancement. In addition, we\ndiscuss persistent challenges, including inconsistencies of named entities annotated across different\ncorpora and the limited availability of named entities of various entity types, particularly for multi-\ntype or nested NER. To the best of our knowledge, this paper is the first comprehensive review of\npre-trained language model-based flat and nested BioNER models, providing a categorical analysis\namong the methods and related challenges for future research and development in the field.\nKeywords: named entity recognition; biomedical named entity recognition; flat named entity\nrecognition ; nested named entity recognition; flat and nested named entity recognition; natural\nlanguage processing\n1. Introduction\nBiomedical articles serve as invaluable resources of knowledge and insights in medicine\nand life sciences. However, the manual extraction of comprehensive information from the\nrapidly amassing literature often comes with prohibitive costs. Consequently, there has\nbeen an increase in interest in developing automated and intelligent systems for extracting\ninformation from biomedical articles, including tasks such as text summarization [ 1,2],\nrelation extraction [ 1,3], and knowledge graph construction [ 4]. Named entity recognition\n(NER) plays a crucial role in biomedical information extraction because it serves as the\nfoundation for various downstream tasks. For instance, in relation extraction, the entities\nrecognized through NER are used to identify and extract meaningful relationships between\nthem [ 5]. Similarly, in knowledge graph construction, NER results form the basis for the\ngraph, where the recognized entities become the nodes and the extracted relationships\nform the edges, allowing for a structured representation of knowledge [6].\nThe NER task aims to identify and classify terms into predefined categories, such\nas diseases, genes, or chemicals [ 7,8]. The NER is categorized into flat and nested NERs\nAppl. Sci. 2024 ,14, 9302. https://doi.org/10.3390/app14209302 https://www.mdpi.com/journal/applsci\nAppl. Sci. 2024 ,14, 9302 2 of 23\nbased on the structure of the target entities. The flat NER task recognizes entities that are a\nsingular span of consecutive words without overlap or nesting with others. On the other\nhand, the nested NER recognizes all named entities (NEs) that either overlap or nest with\nothers, considering the hierarchical properties of entities. The properties provide a more\ncomprehensive understanding of themselves and their interrelationships.\nBiomedical NER (BioNER) poses significant challenges for several reasons [ 9\u201311]. One\nof the challenges is that a single term may encompass multiple types of entities depending\non the context, introducing potential ambiguity. Moreover, biomedical terminologies are\nhighly specialized and often differ from general ones, leading to suboptimal results when\napplied to embedding spaces designed for general corpora. Additionally, the continuous\ncreation of new biomedical terms presents a challenge for BioNER to keep up with the\nlatest vocabulary. In addition, biomedical NEs can cascade, introducing confusion about\nentity boundaries. For example, the DNA region \u201cHIV-2 enhancer\u201d contains the virus \u201cHIV-\n2\u201d. Furthermore, biomedical entities often have descriptive names conveying important\ninformation about their functions, properties, or diagnostic criteria. Examples include\n\u201cclear-cell metastatic renal cell carcinoma\u201d and \u201cautosomal dominant nocturnal frontal lobe\nepilepsy\u201d. These complications have highlighted the importance of nested NER to obtain\nmore correct NEs.\nEarly BioNER systems relied on predefined dictionaries or rules to identify NEs [ 12,13],\nhaving limitations in recognizing NEs beyond the coverage of their predefined repertoires.\nTraditional machine learning (ML) approaches, on the other hand, learn patterns in fea-\ntures associated with NEs, such as capitalization, prefixes, suffixes, and part-of-speech\ntags, allowing them to identify entities with homologous meaning [ 14\u201316]. While these\napproaches can predict previously unseen NEs, they have the drawback of relying on man-\nually designed features, which limits scalability and hinders the discovery of innovative\npatterns within the data.\nIn contrast, deep learning (DL) approaches automatically extract features using learn-\ning algorithms [ 17\u201319]. This allows them to adapt to diverse and heterogeneous entities\nand generalize better to unseen data. In particular, the advent of large-scale pre-trained\nlanguage models (PLMs) such as BERT [ 20], XLNet [ 21], and GPT [ 22] has driven significant\nadvancements in natural language processing (NLP) in recent years. Transfer learning\nmodels with PLMs have achieved state-of-the-art (SOTA) performance on various NLP\ntasks [23,24].\nThese PLM-based models have several limitations. First, PLMs such as BERT are\nconstrained by a maximum input length, making it challenging to capture context in\nlonger texts. Second, large-scale language models such as BERT, which has about 110 mil-\nlion parameters, and XLNet, which has about 117 million parameters, require substantial\ncomputational resources, making deployment difficult in resource-limited environments.\nTo address these issues, models such as Longformer [ 25] extend the input length, while\nlightweight models such as DistilBERT [ 26] and BioMobileBERT [ 27] offer reduced compu-\ntational overhead without sacrificing performance.\nMany studies have adopted PLM in recent years. As shown in Supplementary Table\nS1, these methods have achieved significant improvements compared to existing machine\nlearning and deep learning models, with performance increases of 3\u20137% across datasets\nsuch as BC2GM, NCBI disease, BC5CDR disease, and BC4CHMED. In line with this trend,\nthis study provides a comprehensive review of the recent advancements in both flat and\nnested BioNER, with a focus on PLM-based approaches. This presents relevant resources\nand categorizes models from a problem-solving perspective. The review encompasses\na wide range of approaches, from conventional methods to the latest innovations in the\nfield. In particular, the flat NER section covers sequence labeling, the machine read-\ning comprehension (MRC)-based approach, and multi-task learning for the multi-type\nNER model. Additionally, the nested NER section presents layer-based, span labeling\napproaches, sequence-to-set, and dependency parsing-based approaches. We also provide\nan analysis covering various aspects useful for future research, including the similarities\nAppl. Sci. 2024 ,14, 9302 3 of 23\nbetween different entity types and the contribution of encoding and decoding layers in\nPLM-based NER models. Finally, we discuss persistent issues, such as inconsistencies in\nentity type definitions across datasets and the limited availability of annotated corpora,\nespecially for multi-type or nested NER tasks. To the best of our knowledge, this paper\nserves as the first comprehensive review of PLM-based flat and nested BioNER models.\n2. Background\n2.1. Overview of Flat NER\nThe flat NER identifies NEs defined by a single span of consecutive words without\noverlapping and nesting with other entities [15,28]. For example, as shown in Figure 1a,b,\nthe flat NER model recognizes four entities in the given sentence: \u201cTetracycline resistance\u201d\nas a gene; \u201c Clostridium perfringens \u201d as a species; \u201ccattle\u201d as a species; and \u201cmalignant edema\u201d\nas a disease. A primary approach used in flat NER is sequence labeling, which generates\na sequence of labels for an input sentence, creating one-to-one input-output pairs. The\nlabeling scheme for this approach includes IO, BIO, BIOES, and BILOU, according to the\ncombination of labels [ 29\u201331]. For example, the BIO scheme consists of three labels: B, I,\nand O. The label B indicates the beginning of the NE (i.e., the first word), and the label I\nindicates all other words in the span of the NE. The label O is used for words that are not\npart of any NE. For instance, when annotating a gene \u201cTetracycline resistance genes\u201d using\nthe BIO scheme, \u201cTetracycline\u201d would be labeled as \u201cB-gene\u201d, and both \u201cresistance\u201d and\n\u201cgenes\u201d would be labeled as \u201cI-gene\u201d. Additionally, the BIOES scheme extends the BIO\nscheme by including the label S for a single-word NE and the label E for the last word of\nthe NE.\nAppl. Sci. 2024 , 14, x FOR PEER REVIEW  3 of 24 \n \napproaches, sequence -to-set, and dependency parsing -based approaches. We also provide \nan analysis covering various aspects useful for future research, including the similarities \nbetween different entity types and the contribution of encoding and decoding l ayers in \nPLM-based NER models. Finally, we discuss persistent issues, such as inconsistencies in \nentity type definitions across datasets and the limited availability of annotated corpora, \nespecially for multi -type or nested NER tasks. To the best of our kn owledge, this paper \nserves as the first comprehensive review of PLM -based flat and nested BioNER models.  \n2. Background  \n2.1. Overview of Flat NER  \nThe flat NER identifies NEs defined by a single span of consecutive words without \noverlapping and nesting with other entities  [15,28]. For example, as shown in Figure 1a,b, \nthe flat NER model recognizes  four entities in the given sentence:  \u201cTetracycline resistance\u201d \nas a gene; \u201cClostridium perfringens \u201d as a species; \u201ccattle\u201d as a species; and \u201cmalignant \nedema\u201d as a disease. A primary approach used in flat NER is sequence labeling, which \ngenerates a sequence of labels for an input sentence , creating one -to-one input -output \npairs. The labeling scheme for this approach includes IO, BIO, BIOES, and BILOU, accord-\ning to the combination of labels  [29\u201331]. For example, the BIO scheme consi sts of three \nlabels: B, I, and O. The label B indicates the beginning of the NE (i.e., the first word), and \nthe label I indicates all other words in the span of the NE. The label O is used for words \nthat are not part of any NE. For instance, when annotatin g a gene \u201cTetracycline resistance \ngenes\u201d using the BIO scheme, \u201cTetracycline \u201d would be labeled as \u201cB -gene,\u201d and both \u201cre-\nsistance\u201d and \u201cgenes\u201d would be labeled as \u201cI -gene.\u201d Additionally, the BIOES scheme ex-\ntends the BIO scheme by including the label S for a  single-word NE and the label E for the \nlast word of the NE.  \n \nFigure 1. Example of flat and nested NER  results. (a) Original sentence. ( b) Result from flat NER. ( c) \nResult from nested NER. The yellow rectangle  indicates a gene; the orange rectangle  indicates a \nspecies; the blue rectangle  indicates a disease; and the green rectangle  indicates a chemical as labels.  \n2.2. Overview of Nested NER  \nThe nested NER aims to recognize entities at all  hierarchical  levels, including those \nembedded within other entities [15,32]. For instance, as shown in Figure 1c, the nested \nNER model additionally identifies \u201cTetracycline\u201d as a chemical  and \u201cClostridium \u201d as a spe-\ncies alongside the entities detected by the flat NER model ( Figure 1b). While the sequence \nlabeling approach, which is typically used in the flat NER, can handle nested structures, \nit necessitates a more complex labeling scheme [33].  For this reason , nested NER is com-\nmonly approached using a tuple scheme. Formally, given an input sentence \ud835\udc4b=\n{\ud835\udc651,\ud835\udc652,\u2026,\ud835\udc65\ud835\udc5b} , nested NER outputs all entities in the input sentence as \ud835\udc4c=\nFigure 1. Example of flat and nested NER results. ( a) Original sentence. ( b) Result from flat NER.\n(c) Result from nested NER. The yellow rectangle indicates a gene; the orange rectangle indicates a\nspecies; the blue rectangle indicates a disease; and the green rectangle indicates a chemical as labels.\n2.2. Overview of Nested NER\nThe nested NER aims to recognize entities at all hierarchical levels, including those\nembedded within other entities [ 15,32]. For instance, as shown in Figure 1c, the nested\nNER model additionally identifies \u201cTetracycline\u201d as a chemical and \u201c Clostridium \u201d as a\nspecies alongside the entities detected by the flat NER model (Figure 1b). While the se-\nquence labeling approach, which is typically used in the flat NER, can handle nested\nstructures, it necessitates a more complex labeling scheme [ 33]. For this reason, nested\nNER is commonly approached using a tuple scheme. Formally, given an input sen-\ntence X={x1,x2, . . . , xn}, nested NER outputs all entities in the input sentence as\nY=n\u0010\nshead\n1,stail\n1,t1\u0011\n,\u0010\nshead\n2,stail\n2,t2\u0011\n, . . . ,\u0010\nshead\nm,stail\nm,tm\u0011o\n. Here, nis the number\nof words in the sentence, and mis the number of NEs. In the output, shead\niand stail\nirepre-\nsent indices of the head and tail words of the i-th NE, respectively, and tirepresents the\ncorresponding type of NE. For example, when annotating the nested NEs in Figure 1c, the\nAppl. Sci. 2024 ,14, 9302 4 of 23\ntuples would be represented as follows: {(1, 1, chemical), (1, 3, gene), (5, 5, species), (5, 6,\nspecies), (9, 9, species), (12, 13, disease)}.\n2.3. Evaluation Metrics for NER\nThree metrics are commonly used to evaluate performance in NER: precision, recall,\nand F1-score [ 28,34]. Precision is the proportion of correctly identified entities among the\nNEs predicted by the model, which is calculated as\nprecision =TP\nTP+FP, (1)\nand recall is the proportion of NEs correctly predicted by the model among the actual NEs,\nwhich is calculated as\nrecall =TP\nTP+FN(2)\nHere, TPrefers to the number of NEs that are correctly predicted by the model. FP\nrefers to the number of NEs incorrectly predicted. FNrefers to the number of actual NEs\nthat the model fails to predict. In the NER, a predicted NE is considered correct if it matches\nthe actual NE in both its boundaries and type annotations.\nThe F1-score is a comprehensive metric that balances precision and recall. It is calcu-\nlated as:\nF1-score =2\nprecision\u22121+recall\u22121=2\u00d7precision \u00d7recall\nprecision +recall. (3)\nThe highest achievable value for the F1-score is 1.0, with higher F1-scores indicating\nbetter overall efficiency of the model.\n2.4. Text Representations\nText representation is a primary component of NLP models. Well-trained text represen-\ntation positively impacts NLP model performance. Low-dimensional word representations,\ncommonly called word embeddings [ 35,36], have been demonstrated to be effective across\na variety of NLP tasks. However, word-level representations suffer from a chronic out-of-\nvocabulary (OOV) problem. Alternative approaches, such as fastText [ 37] and character\nembedding [ 38], have been developed to address this issue. FastText learns representations\nfor n-gram characters and represents a word by aggregating the vectors of its constituent\nn-grams. The character embedding model learns to represent each character in a word as a\nvector and then combines these character vectors to obtain a representation for the entire\nword. Both fastText and character embedding mitigate the OOV problem by capturing the\nmorphological aspects of words.\nThese representations are typically pre-computed and remain static during the training\nand inference of the NLP model. As a result, these static representations treat different\nhomonyms as the same vector, ignoring their contextual differences. In contrast, contextual\nlanguage models, such as BERT [ 20], ELMo [ 39], and GPT [ 22], dynamically capture word\nsemantics in a context-dependent manner using DL. Specifically, they assign different vec-\ntors to the same word depending on the context, thereby solving the problem of homonyms.\nMoreover, these contextual language models can be transfer-learned for downstream tasks.\nThe transfer learning models based on the contextual language model have achieved SOTA\nperformance in a variety of domains. In the biomedical field, biomedical PLMs such as\nClinicalBERT [40], PubmedBERT [41], and Bioformer [42] are widely utilized.\n2.5. Biomedical NER Datasets\nTable 1 presents benchmark datasets comprising 26 and 2 for flat and nested NE\ndatasets. The table also provides whether NE normalization labels are included in each\ndataset. The normalization task maps NEs to unique, standardized identifiers to ensure that\nsimilar or identical entities are recognized consistently. This task is an essential post-process\nfor NER as it enhances the clarity and usability of NEs.\nAppl. Sci. 2024 ,14, 9302 5 of 23\nTable 1. List of datasets for flat and nested named entity recognition.\nDataset Norm+URL Ref.\nFlat NER\nMulti-Type\nBioRED \u2713 https://ftp.ncbi.nlm.nih.gov/pub/lu/BioRED/ [43]\nMedMention \u2713 https://github.com/chanzuckerberg/MedMentions [44]\nCRAFT \u2713 http://bionlp-corpora.sourceforge.net/CRAFT/ [45]\nJNLPBA https://github.com/openbiocorpora/jnlpba * [46]\nCellFinder https://github.com/openbiocorpora/cellfinder * [47]\nmiRNA https://www.scai.fraunhofer.de/mirna-corpora.html [48]\nNagelhttps://sourceforge.net/projects/bionlp-corpora/files/\nProteinResidue/[49]\nGREC http://www.nactem.ac.uk/GREC/ [50]\nBC5CDR \u2713 https://github.com/openbiocorpora/biocreative-v-cdr * [51]\nDrugProt https://zenodo.org/records/5119892 [52]\ntmVar 3.0 \u2713 https://github.com/ncbi/tmVar3?tab=readme-ov-file [53]\nGene/Protein\nBC2GM \u2713 https://github.com/openbiocorpora/biocreative-ii-gm * [54]\nDECA \u2713 http://www.nactem.ac.uk/deca/ [55]\nGETM https://getm-project.sourceforge.net/ [56]\nLocText \u2713 https://github.com/Rostlab/LocText [57]\nFSU-PRGE https://julielab.de/Resources/FSU_PRGE.html [58]\nNLM-Gene \u2713 https://ftp.ncbi.nlm.nih.gov/pub/lu/NLMGene/ [59]\nChemical\nSCAI Chem \u2713 https://www.scai.fraunhofer.de/chem-corpora.html [60]\nDDI https://github.com/isegura/DDICorpus [61]\nBC4CHEMD \u2713 https://github.com/bionlp-hzau/BioNLP-Corpus * [62]\nNLM-Chem \u2713 https://ftp.ncbi.nlm.nih.gov/pub/lu/NLMChem/ [63]\nDisease\nSCAI disease https://www.scai.fraunhofer.de/disease-ae-corpus.html [64]\nNCBI disease \u2713 http://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/ [65]\nSpecies\nLINNAEUS \u2713 http://linnaeus.sourceforge.net/ [66]\nSpecies-1000 \u2713 https://jensenlab.org/resources/s1000/ [67]\nVariant\nSNP corpus \u2713 https://www.scai.fraunhofer.de/snp-normalization-corpus.html [68]\nNested NER\nGENIA https://github.com/openbiocorpora/genia-term * [69]\nBacteria biotopes \u2713 https://sites.google.com/view/bb-2019/dataset?authuser=0 [70]\n+Norm indicates NE normalization. * These URLs are not from official sources; they have been shared by\nindependent users. URLs accessed on 14 December 2023.\nAmong datasets, 11 flat and 2 nested NE datasets involve multiple entity types. The\nmulti-type datasets embody realistic complexity and diversity, thereby enhancing the\ncapabilities of NER models. Moreover, GENIA and bacteria biotopes (BB19) provide\nnested entities in approximately 22% and 11% of sentences in each set, respectively. Such\nnested NE datasets allow NER models to learn hierarchical information representing the\ncomplicated relationships among biomedical entities.\n3. Deep Learning Methods for Flat NER\nThis section categorizes flat BioNER models into three main approaches: sequence\nlabeling, MRC, and multi-task learning. Sequence labeling models are favored for their\nsimplicity and efficiency, making them straightforward to implement and fast. MRC\nmodels enhance performance by integrating external knowledge and framing NER tasks\nas question-answering problems. Multi-task learning models predict multiple types of\nentities simultaneously by leveraging shared knowledge across tasks. Table 2 reports the\nperformance metrics of each model on biomedical corpora, with the metrics sourced from\nAppl. Sci. 2024 ,14, 9302 6 of 23\nthe respective method papers. While the recent models achieve high F1-scores (90\u201393%)\nfor NCBI disease, BC5CDR chemical, BC4CHEMD, and LINNAEUS, they have relatively\nweak performance on BC2GM (84.4 \u00b11.2%) and BC5CDR disease (87.7 \u00b10.5%). The gene\nentities in BC2GM include complex variations such as abbreviations, numbers, and special\ncharacters (e.g., pepX, AB004534, and SNAP-23), adding challenges for models. Similarly,\nBC5CDR disease shows weaker performance than NCBI disease, likely due to differences\nin disease scope and low overlap in linguistic content between training and test data [71].\nHereafter, models are introduced based on a two-step structure consisting of encoding\nand decoding layers, as illustrated in Figure 2. The encoding layer includes a text vectoriza-\ntion layer and a context layer. The text vectorization layer converts words or tokens into\nnumerical vectors. The context layer enhances the ability of the model to understand the\nbroader linguistic context by employing neural networks, such as long short-term memory\n(LSTM) or transformer-based language models. Lastly, the decoding layer aims to predict\nNEs based on context information from the encoding layer. The configuration of this layer\nvaries depending on the specific approach to handling NEs.\nAppl. Sci. 2024 , 14, x FOR PEER REVIEW  7 of 24 \n \n \nFigure 2. Overview of three flat NER approaches.  For each approach, the diagram shows how an \ninput sequence {\ud835\udc651,\u2026,\ud835\udc65\ud835\udc5b}, where \ud835\udc65\ud835\udc56 indicates the \ud835\udc56-th word in an input sentence,  is transformed \nthrough encoding and decoding layers to predict flat NEs. The encoding layer consists of a text \nvectorization, which converts words or tokens into numerical vectors, and a context layer that uses \nneural networks such as LSTM or transformer to capture broader linguistic context. The decoding \nlayer then predicts NEs based on the context from the encoding layer.  (a) Sequence labeling ap-\nproach. \u210e\ud835\udc56 and \u210e\u0302\ud835\udc56 refer to the initial and latent embeddings for \ud835\udc65\ud835\udc56, respectively. \ud835\udc66\ud835\udc56 is a label as-\nsigned to \ud835\udc65\ud835\udc56. (b) MRC-based approach . This approach includes an additional input in the form of a \nquery: \ud835\udc5e\ud835\udc56 represents the \ud835\udc56-th word in the query. \u210e\ud835\udc5e\ud835\udc56 indicates initial embedding for \ud835\udc56-th word in \nthe query, while \u210e\u0302\ud835\udc5e\ud835\udc56 represents its latent embedding. \ud835\udc46\ud835\udc5d\ud835\udc4e \ud835\udc5b\ud835\udc56 refers to \ud835\udc56-th entity generated through \nboundary detection. ( c) Multi-task learning approach. \ud835\udc4c\ud835\udc61\ud835\udc4e\ud835\udc60\ud835\udc58\ud835\udc56 is the output for the \ud835\udc56-th task.  \n3.1. Sequence Labeling  \nThe sequence labeling approach is the most commonly used method in flat NER ( Fig-\nure 2a). In this approach, a sentence is split into individual tokens , which are taken as \ninput and processed through an encoding layer. The encoding layer enables the model to \ninterpret each token within the context of the entire sentence. Following this, a token clas-\nsification layer is applied to assign a label to each token based on the contextual in for-\nmation learned from the previous layer.  \nLee et al. [23] utilize a transformer model, trained on the biomedical text and named \nBioBERT, in the encoding layer. They also employ a feed -forward neural network (FFNN) \nand softmax function as the token classification layer. BioBERT is initialized with the \nFigure 2. Overview of three flat NER approaches. For each approach, the diagram shows how an\ninput sequence {x1, . . . , xn}, where xiindicates the i-th word in an input sentence, is transformed\nAppl. Sci. 2024 ,14, 9302 7 of 23\nthrough encoding and decoding layers to predict flat NEs. The encoding layer consists of a text\nvectorization, which converts words or tokens into numerical vectors, and a context layer that uses\nneural networks such as LSTM or transformer to capture broader linguistic context. The decoding\nlayer then predicts NEs based on the context from the encoding layer. ( a) Sequence labeling approach.\nhiand \u02c6hirefer to the initial and latent embeddings for xi, respectively. yiis a label assigned to xi.\n(b) MRC-based approach. This approach includes an additional input in the form of a query: qi\nrepresents the i-th word in the query. hqiindicates initial embedding for i-th word in the query, while\n\u02c6hqirepresents its latent embedding. Span irefers to i-th entity generated through boundary detection.\n(c) Multi-task learning approach. Ytask iis the output for the i-th task.\nTable 2. A comprehensive list of methods for flat BioNER.\nApproach\nTypeModelPerformance (F1-Score)\nCode Ref.Gene/Protein Disease Chemical Species\nBC2GMNCBI\nDiseaseBC5CDR\nDiseaseBC5CDR\nChemicalBC4CHEMD LINNAEUS\nSequence\nlabelingBioBERT 84.72 89.71 87.15 93.47 92.36 88.24 Yes [23]\nNaseem et al. 86.05 91.23 88.34 94.24 92.28 - No [24]\nMRCBioBERT-\nMRC85.48 90.04 87.83 94.19 92.92 - Yes [72]\nMulti-task\nlearningMT-BioNER 83.01 88.10 - 89.50 - - No [73]\nMTL-LS 82.92 89.25 87.28 93.83 92.42 86.37 No [74]\nBERT-CNN 83.47 89.72 - - 92.39 92.63 Yes [75]\nAIONER - 89.59 87.89 92.84 - 90.63 Yes [76]\nTaughtNet 84.84 89.20 - 93.95 - - Yes [77]\n3.1. Sequence Labeling\nThe sequence labeling approach is the most commonly used method in flat NER\n(Figure 2a). In this approach, a sentence is split into individual tokens, which are taken\nas input and processed through an encoding layer. The encoding layer enables the model\nto interpret each token within the context of the entire sentence. Following this, a token\nclassification layer is applied to assign a label to each token based on the contextual\ninformation learned from the previous layer.\nLee et al. [ 23] utilize a transformer model, trained on the biomedical text and named\nBioBERT, in the encoding layer. They also employ a feed-forward neural network (FFNN)\nandsoftmax function as the token classification layer. BioBERT is initialized with the weights\nof BERT [ 20] and retrained on the biomedical domain corpora, including PubMed abstracts\nand PMC full-text articles. It outperforms BERT on most BioNER datasets, proving the\neffect of the domain-specific representation model.\nA model developed by Naseem et al. [ 24] employs a fused representation by con-\ncatenating four representations: word-level representation [ 78], character-level representa-\ntion [ 31], BioELMo [ 79], and BioBERT [ 23]. They apply an attention-based bi-directional\nLSTM (Bi-LSTM) as the context layer and a conditional random field (CRF) as the token\nclassification layer. The fused representation outperforms the one using BioBERT alone.\nHowever, the complex representation requires substantial computational resources and\nmemory, which may hinder its efficiency in real-time processing and resource-constrained\nenvironments.\n3.2. Machine Reading Comprehension-Based Approach\nMRC is an automated system designed to comprehend a passage and respond to\nquestions. This approach has been applied in various NLP tasks. Figure 2b illustrates\nthe process of an MRC-based approach designed for the NER task. This approach takes\na sentence along with a query as input. The queries include information about the NEs,\nAppl. Sci. 2024 ,14, 9302 8 of 23\nsuch as entity type, definitions, and examples. Through the encoding layer, the sentence\u2019s\nembeddings incorporate the information from the query. Finally, in the boundary detection\nlayer, the model identifies a set of spans corresponding to NEs.\nBioBERT-MRC, as proposed by Sun et al., uses the MRC approach to address the flat\nNER problem [ 72]. Their input is defined as \u201c[CLS] Sentence [SEP] Query [SEP]\u201d, where\n[CLS] and [SEP] are special tokens used to indicate the start of input and the separation\nbetween sentences, respectively. The query involves a request to detect NEs along with\nexamples of such entities. The BioBERT is used for the encoding layer. In the boundary\ndetection layer, two FFNNs are employed to predict the start and end indices of NEs.\nThese two FFNNs operate sequentially, first predicting the start indices and then predicting\nthe end indices with information on the start indices. Finally, the model generates final\noutputs by matching the nearest start and end indices. This model is limited in that it\ncan only predict a single entity type per inference, even when using multi-type datasets.\nMoreover, the presence of queries may aggravate the inherent length limitations in PLM-\nbased models. Nevertheless, the approach of incorporating external knowledge into the\nmodel through queries is interesting. The MRC system can also be utilized to solve nested\nor multi-type NEs.\n3.3. Multi-Task Learning\nA straightforward approach to enabling an NER model to predict diverse types of\nentities is to train it on a dataset that encompasses all those types of entities. However,\nmulti-type datasets composed of the desired entity types are rare in the biomedical do-\nmain. To overcome such limitations, several studies have explored methods to train a\nmodel on a combination of multiple datasets. A representative approach is multi-task\nlearning, which successfully achieves this goal by training a model to perform multiple\ntasks simultaneously.\nFigure 2c illustrates the general multi-task learning model consisting of a single shared\nlayer and several task-specific layers. The shared layer is used for all tasks and learns\ncommon knowledge across them. Subsequently, the latent embedding, processed through\nthe share layer, is passed onto the task-specific layers, which are further refined and\nspecialized for each task, ultimately producing task-specific outputs\b\nYtask 1,. . . , Ytask n\t\ntailored to each task. Given mtasks, for i\u2208{1, . . . , m}, the total loss function Ltotalof the\nmulti-task model can be expressed as [73]\nLtotal =\u2211m\ni=1\u03bbiLi(\u03b8shared ,\u03b8i). (4)\nHere, the hyperparameter \u03bbicontrols the contribution of the i-th task. The param-\neters in the shared layer and the i-th task-specific layer are represented by \u03b8shared and\u03b8i,\nrespectively.\nKhan et al. introduced MT-BioNER, which utilizes an encoding layer (i.e., BioBERT)\nas a shared layer and a decoding layer as a task-specific layer [ 73]. The task-specific layers\nare assigned to each dataset. They present two versions of the model: one trained on three\ndatasets (BC2GM, BC5CDR chemical, and NCBI disease) and the other trained on four\ndatasets, including JNLPBA. The latter model, with additional training data, performs\nworse overall than the former. This result suggests that merely augmenting the training\ndata does not necessarily enhance performance. Similarly, Chai et al. proposed MTL-\nLS, which consists of a shared layer and multiple task-specific layers [ 74]. They apply\nhierarchical sharing on the encoding layer to improve the stability of the multi-task model.\nSpecifically, they divide the encoding layer (i.e., XLNet [ 21]) into the lower and upper parts.\nThe lower part serves as a shared layer, while the upper part, along with the decoding layer,\nis allocated per task. Note that MTL-LS was trained on multiple datasets by categorizing\nthem based on entity types, with each category treated as a separate task.\nMeanwhile, Banerjee et al. introduced a multi-task learning model using the MRC\nsystem, named BERT-CNN [ 75]. It is noted that this model receives information about\ntasks through queries. Therefore, unlike typical multi-task learning models, it does not\nAppl. Sci. 2024 ,14, 9302 9 of 23\nrequire task-specific layers and has the structure of the MRC-based model (Figure 2b). The\nmodel utilizes a combination of BERT and a convolutional neural network (CNN) as an\nencoding layer, which captures global and local information, respectively. Subsequently,\nthe model utilizes a sequence labeling layer to determine the boundaries of certain types of\nNEs. The labeling scheme includes \u201cB\u201d, \u201cI\u201d, and \u201cO\u201d without specifying the type of entity.\nSimilarly, all-in-one NER (AIONER), as developed by Luo and Wei, receives information\nabout tasks through special tags instead of queries [ 76]. The special tags, unlike the queries\nused in BERT-CNN, do not incorporate detailed external information such as definitions or\nexamples. The input for AIONER is structured as \u201c<task> Sentence </task>\u201d, where the\ntag pair \u201c<task></task>\u201d specifies a particular entity type, such as \u201c<disease></disease>\u201d.\nAIONER also employs a sequence labeling layer to predict the boundaries of NEs. For\na particular task, its labeling scheme consists of \u201cB-task\u201d, \u201cI-task\u201d, and \u201cO-task\u201d labels\n(e.g., B-disease, I-disease, and O-disease). Unlike traditional BIO schemes, \u201cO\u201d labels are\ncustomized for each task to enhance flexibility and alleviate task conflicts. This model also\nsupports the \u201c<all></all>\u201d tag pair, which collectively represents all entity types. However,\nthis tag requires a dataset that includes all entity types.\nMoscato et al. [ 77] proposed TaughtNet, a novel method of multi-task learning that\napplies knowledge distillation. Knowledge distillation involves training a compact student\nmodel to emulate a larger teacher model. They first train single-type NER models to serve\nas teachers on distinct datasets. In this study, three teacher models are trained separately\non the NCBI disease, BC5DER chemical, and BC2GM. Subsequently, they integrate the\nprobability distributions from teacher models into a unified distribution that the student\nmodel can emulate. In the training phase, the student model aims to minimize the dis-\ncrepancy between its own distribution and the unified distribution. Simultaneously, it\nalso minimizes the discrepancy with ground truth, similar to existing sequence labeling\nmodels. The knowledge distillation approach enables multi-task learning on a model that\nis more compact than the single-teacher model, highlighting its usefulness under stringent\ncomputational and memory constraints.\nThe models under discussion differ in their approach to integrating task-specific\ninformation and in their architectural structure. MT-BioNER and MTL-LS employ a shared\nencoding layer with task-specific decoding layers for each dataset of entity types. This\nresults in a more complex structure due to the necessity for discrete decoders. In contrast,\nBERT-CNN, AIONER, and TaughtNet eliminate the necessity for task-specific layers. BERT-\nCNN and AIONER acquire task-specific information through queries and tags, respectively,\nallowing them to process multiple tasks within a unified structure. Similarly, TaughtNet\nalso bypasses the need for task-specific layers by training a compact student model to\nemulate several teacher models through knowledge distillation.\nA multi-task learning approach is an effective approach for addressing the lack of\nmulti-type datasets in the BioNER domain. Additionally, it reduces memory requirements\nby allowing multiple tasks to be processed with a single model. However, not all tasks\npositively influence each other, and conflicts between tasks may lead to performance degra-\ndation. Additionally, most multi-task models produce separate outputs for each entity type,\nnecessitating a post-processing step to integrate these outputs for practical applications.\n4. Deep Learning Methods for Nested NER\nNested NER is a more complex problem than flat NER because it recognizes entities\nthat nest or overlap with other entities. Recent advances in DL have yielded promising\nresults in handling nested NER, leading to an increased interest in it. This section provides\nan overview of various nested NER models, broadly classified into layer-based, span\nlabeling, and other approaches, as outlined in Table 3. The layer-based models solve the\nproblem of nested entities by using multiple layers to capture different levels of entity\nnesting. The span labeling approaches identify and classify candidate spans (that are likely\nto be entities) within the text. These approaches are classified into enumeration, boundary\ndetection, and MRC, depending on the identification method of candidate spans. Finally,\nAppl. Sci. 2024 ,14, 9302 10 of 23\nthe other approaches involve models that tackle the nested NER problem in unconventional\nways, such as sequence-to-set and affine models.\nDue to the limited number of biomedical nested NER datasets, Table 3 includes\nperformance metrics not only for the GENIA [ 69] and BB19 [ 70] datasets but also for three\nadditional datasets\u2014ACE04 [ 80], ACE05 [ 81], and KBP17 [ 82]. The performance metrics\nwere taken from the respective method papers. The recent nested NER models demonstrate\nan average F1-score of 80.8% ( \u00b11.6) on the GENIA dataset. On ACE04 and ACE 05, they\nachieve F1-scores of 87.2% ( \u00b11.1) and 86.4% ( \u00b12.1), respectively, and perform at an average\nof 83.7% ( \u00b11.6) on the KBP17 dataset. BB only has performance results reported by SpanMB,\nwith an F1-score of 81.8%. Additionally, it can be observed that the models show overall\nlower performance on the GENIA corpus compared to the ACE04 and ACE05 datasets,\nwhich involve more general terms.\nTable 3. A comprehensive list of methods for nested NER.\nApproach Type MethodPerformance (F1-Score)\nCode Ref.GENIA BB19 ACE04 ACE05 KBP17\nLayer-basedMerge Label -+- - 82.40 - Yes [83]\nPyramid 79.19 - 86.28 84.66 - Yes [84]Span labelingEnumerationPURE - - 88.10 88.70 - Yes [85]\nSpanMB - 81.80 - - - Yes [86]\nPL-Marker - - 88.80 89.80 - Yes [87]\nBoundaryBENSC 78.30 - 85.30 83.90 - No [88]\nLocate-and-Label 80.54 - 87.41 86.67 84.05 Yes [89]\nMRCBERT-MRC 83.75 - 85.98 86.88 80.97 Yes [90]\nPIQN 81.77 - 88.14 87.42 84.50 Yes [91]OthersSequence-to-setSequence-to-Set 80.44 - 87.26 87.05 83.96 Yes [92]\nPnRNet 81.85 - 88.12 87.63 85.27 Yes [93]\nAffineBiaffine 80.50 - 86.70 85.40 - Yes [94]\nTriaffine 81.23 - 87.40 86.82 - Yes [95]\n+\u2014means that the model does not report performance for the given dataset.\nThis section also introduces models based on a two-step structure consisting of an\nencoding and decoding layer, similar to those described in Section 3.\n4.1. Layer-Based Approaches\nThe layer-based methods handle the hierarchy of nested entities by stacking multiple\nNER layers, designed to predict entities of a specific length or level (Figure 3a). The model\nprogressively predicts entities by stacking layers, starting either from the layer that predicts\nthe shortest (innermost) entities to the one that predicts the longest (outermost) entities, or\nvice versa. For instance, assume a model structured to predict entities from the innermost\nto the outermost levels. The NER layer 1 identifies entities that do not embed other entities\nwithin them. Following this, the NER layer 2 detects entities that embed those identified by\nthe preceding layer. This process continues for subsequent layers, enabling the model to\ncover progressively broader spans of entities.\nFisher and Vlachos proposed Merge and Label, a layer-based model that identifies a\nset of entities by level [ 83]. The model determines the entity spans by assessing whether\nadjacent tokens and/or entities belong to the same entity, assigning a continuous real value\nto this prediction. The predicted real values are used as weights for the words and/or\nentities within the span, enabling a weighted sum calculation to derive the span embedding.\nThese embeddings are then used to classify the spans. The model proceeds by passing\ntokens and/or spans to subsequent layers, thereby enabling the gradual formation of\nlarger spans.\nAppl. Sci. 2024 ,14, 9302 11 of 23\nAppl. Sci. 2024 , 14, x FOR PEER REVIEW  11 of 24 \n \nThis section also introduces models based on a two -step structure consisting of an \nencoding and decoding layer, similar to those described in Section 3 . \n4.1. Layer-Based Approaches  \nThe layer -based methods handle the hierarchy of nested entities by stacking multiple \nNER layers, designed to predict entities of a specific length or level ( Figure 3a). The model \nprogressively predicts entities by stacking layers, starting either from the layer that pre-\ndicts the shortest (innermost) entities to the one that predicts the longest (outermost) en-\ntities, or vice versa. For instance, assume a model structured to predict entities from the \ninnermost to the outermost levels. The NER layer 1 identifies entities that do not embed \nother entities within them. Following this, the NER layer 2 detects entities that embed \nthose identified by the preceding layer. This process continu es for subsequent layers, en-\nabling the model to cover progressively broader spans of entities.  \n \nFigure 3. Overview of three approaches for nested NER. For each approach, the diagram shows how \nan input sequence {\ud835\udc651,\u2026,\ud835\udc65\ud835\udc5b}, where \ud835\udc65\ud835\udc56 indicates the \ud835\udc56-th word in an input sentence,  is transformed \nthrough encoding and decoding layers to predict nested NEs. The encoding layer consists of a text \nvectorization, which converts words or tokens into numerical vectors, and a context layer that uses \nneural networks such as LSTM or transformer to capture broader linguistic context. The decoding \nlayer then predicts NEs based on the context from the encoding layer.  (a) Layer-based approach. \u210e\ud835\udc56 \nand \u210e\u0302\ud835\udc56  refer to the initial and latent embeddings for \ud835\udc65\ud835\udc56 , respectively. \ud835\udc4c\ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc52 \ud835\udc5f\ud835\udc56  denotes the output \nfrom \ud835\udc56-th NER layer. ( b) Enumeration model of span -based approach. \u210e\u0302(\ud835\udc56,\ud835\udc57) indicates the latent em-\nbedding for the span from \ud835\udc65\ud835\udc56 to \ud835\udc65\ud835\udc57, and \ud835\udc66(\ud835\udc56,\ud835\udc57) refers to a class assigned to that span. ( c) Boundary \nmodel of span -based approach. \u210e\u0302\ud835\udc60\ud835\udc56 represents the latent embedding for \ud835\udc56-th candidate span deter-\nmined by the boundary detection layer. \ud835\udc66\ud835\udc60\ud835\udc56 denotes the label assigned to the \ud835\udc56-th candidate span.  \nFigure 3. Overview of three approaches for nested NER. For each approach, the diagram shows how\nan input sequence {x1, . . . , xn}, where xiindicates the i-th word in an input sentence, is transformed\nthrough encoding and decoding layers to predict nested NEs. The encoding layer consists of a text\nvectorization, which converts words or tokens into numerical vectors, and a context layer that uses\nneural networks such as LSTM or transformer to capture broader linguistic context. The decoding\nlayer then predicts NEs based on the context from the encoding layer. ( a) Layer-based approach. hi\nand \u02c6hirefer to the initial and latent embeddings for xi, respectively. Ylayer idenotes the output from\ni-th NER layer. ( b) Enumeration model of span-based approach. \u02c6h(i,j)indicates the latent embedding\nfor the span from xitoxj, and y(i,j)refers to a class assigned to that span. ( c) Boundary model of\nspan-based approach. \u02c6hsirepresents the latent embedding for i-th candidate span determined by the\nboundary detection layer. ysidenotes the label assigned to the i-th candidate span.\nWang et al. developed a layered model pyramid, which predicts complete entity\nmentions of the corresponding length in each layer [ 84]. The model consists of multiple\ninterconnected layers and employs a convolutional network with two kernels between\nthe layers to combine information from adjacent words and/or spans, forming a pyramid\nstructure. Furthermore, the model operates in both forward and backward directions\nthrough an inverse pyramid, providing bidirectional interactions between adjacent layers.\nThis model alleviates common issues of the general layer-based approaches, such as layer\ndisorientation and error propagation. These studies demonstrate that the hierarchical\nstructures of nested NEs can be handled in intuitive ways.\n4.2. Span Labeling Approaches\nThe span labeling approach addresses the nested NER problem by assigning labels to\nall candidate spans likely to be NEs (Figure 3b,c). This approach detects all NEs simultane-\nously, mitigating the error propagation problem. In this section, we classify span labeling\nAppl. Sci. 2024 ,14, 9302 12 of 23\napproaches into three categories: enumeration, boundary, and MRC. The enumeration and\nboundary models are distinguished by how they generate candidate spans. The enumera-\ntion model sets all spans shorter than a certain length as candidates, whereas the boundary\nmodel uses neural networks to generate candidate spans. The MRC model differs from the\nother two by incorporating information about entity type using queries.\n4.2.1. Enumeration Model\nThe enumeration model considers all spans shorter than a threshold as potential NE\ncandidates (Figure 3b). All NE candidates are transformed into high-quality representations\nbased on the span representation strategy and then classified as either specific entity types or\nnon-entities through a span classification layer. The enumeration model is a straightforward\napproach that can consider all valid spans. Consequently, its performance heavily depends\non the quality of the span representation.\nZhong and Chen define a span representation as a concatenation of the head and tail\ntoken representations, along with a learned representation of its width feature [ 85]. This\nconcatenation strategy has the disadvantage of ignoring information from intermediate\ntokens within the span. To overcome the limitation, Zuo et al. employ a max-pooling\nrepresentation [ 86]. The max-pooling representation is defined as collecting the maximum\nvalues for each dimension across the token representations within the span. It is then\nconcatenated with the representations of the head token, the tail token, and the width\nfeature to construct the span representation.\nThe concatenation and pooling representations are defined by combining token rep-\nresentations generated by the encoding layer. However, these strategies have limitations\nin accounting for dependencies among spans. Ye et al. emphasized the importance of\nthese interrelationships and introduced a packed levitated marker (PL-Marker) [ 87]. In\ntheir model, each span is assigned two levitated markers that contain the position infor-\nmation of its head and tail tokens, respectively. These markers are paired and appended\nto the end of a sentence with other levitated marker pairs from adjacent spans together.\nThe sentence, along with the group of levitated markers, is passed through the encoding\nlayer. Subsequently, the representations of the encoded head and tail levitated markers are\nconcatenated together and utilized as a span representation.\nEnumeration approaches can consider as many spans as possible. However, this\napproach is inherently expensive due to the large number of spans processed. The represen-\ntation strategies that account for dependencies between spans, such as PL-marker, further\nincrease computational and time costs. Additionally, these approaches are constrained by\ntheir inability to predict long entities that exceed a certain threshold. While enumeration\nmethods can intuitively handle nested NEs, their critical disadvantage lies in the substantial\nresource requirements, making them less efficient for time-sensitive applications.\n4.2.2. Boundary Model\nThe boundary approach involves additional layers, called boundary detection, to\ngenerate NE candidates, as illustrated in Figure 3c. These candidates are then assigned\nlabels through the span classification layer. In this section, we discuss the research by\nfocusing on the additional component, the boundary detection layer.\nTan et al. introduced a boundary-enhanced neural span classification (BENSC) model\nthat employs two token classifiers within the boundary detection layer [ 88]. The two\ntoken classification layers predict whether the token is a head or tail word of the entity,\nrespectively. Then, all valid head-tail pairs, i.e., where the tail follows the head, are labeled\nthrough the span classification layer. This model has the advantage of being able to predict\nlonger entities, as it does not impose a length limitation. However, it presents the issue of\nerror propagation, where inaccuracies in the boundary detection layer carry over to the\nnext stages.\nIn general, span-based models treat prediction results that do not exactly match\nthe correct span boundaries as incorrect. However, Shen et al. argue that this rigid\nAppl. Sci. 2024 ,14, 9302 13 of 23\nconsideration can introduce noise into the model [ 89]. To address this issue, they introduced\nboundary regression, inspired by object detection techniques in computer vision. Their\nmodel enumerates all spans that are shorter than a specific threshold and assigns them as\ncandidate spans. The candidates go through two steps: filtration and refinement. In the\ninitial step, the low-quality candidates that do not closely overlap with the actual NEs are\nfiltered out using a binary classification layer. In the subsequent step, the boundaries of\nthe remaining high-quality candidates are refined using a regression layer. The candidate\nspans adjusted in the two steps are finally classified through a span classification layer.\nThis model is not free from the length limitation, as it employs the enumeration strategy for\ngenerating candidates in the initial stage. Although the excessive computational resource\nconsumption associated with enumeration is mitigated through the filtration step, this\nintroduces an error propagation issue. Nevertheless, the model remains notable for its use\nof approximate boundary refinement, allowing for adjustments in span boundaries even\nwhen initial predictions are not perfectly accurate.\nThe primary advantage of these boundary models lies in their ability to narrow the can-\ndidate pool, thereby reducing computational time complexity and resource consumption.\nAlthough its overall performance may be lower compared to the enumeration approach,\nthe model\u2019s resource efficiency makes it well-suited for real-world applications where\ncomputation constraints are a key consideration.\n4.2.3. MRC\nThe MRC-based approaches typically receive information about the entity type to be\npredicted in the form of a query along with the sentence and then predict the boundary of\nthe corresponding type (Figure 2b).\nBERT-MRC utilizes the queries in natural language format [ 90]. The model passes\nthe concatenation of the sentence with the query through an encoding layer, allowing the\ninformation from the query to be integrated into the sentence representation. BERT-MRC\nemploys three binary classifiers to handle nested NEs. The two classifiers are designed\nto identify candidates for head and tail tokens of NEs, respectively. Subsequently, the\nlast classifier determines whether each head-tail pair is an appropriate target for the\nquery. BERT-MRC generates each query for one specific entity type, encountering the\nlimitation that it can handle only one entity type per inference. Therefore, it fails to consider\ninterrelationships among entities of different types. Moreover, the model involves a manual\nprocess for crafting appropriate queries for each entity type, adding to its complexity.\nTo overcome these shortcomings, Shen et al. designed the parallel instance query\nnetwork (PIQN) [ 91]. PIQN employs instance queries that are learnable vectors, diverging\nfrom the traditional natural language form. These instance queries are initialized with\nrandom values and autonomously learn their semantic significance during the training\nphase. The sentence is encoded independently, whereas the instance queries interact with\nthe sentence\u2019s embedding through cross-attention mechanisms. This allows the queries\nto extract relevant contextual information without altering the original sentence encoding.\nPIQN utilizes query embeddings to identify and classify NEs instead of using sentence\nembeddings. For each query, two separate linear layers are used to predict the start and end\npositions of the span corresponding to the query. After determining the span boundaries,\na span classification layer is applied to categorize the span into a specific entity type. In\nother words, each query is designed to identify only a single entity, ensuring that one query\ncorresponds to one entity prediction. The process for multiple queries occurs in parallel,\nallowing the model to efficiently handle multiple entities within the same sentence.\nThis section introduces two models that utilize different query formats: traditional\nnatural language queries and learnable vector formats. BERT-MRC, which employs tradi-\ntional natural language queries, provides the model with precise, user-defined information.\nHowever, the manual crating of these queries requires domain-specific expertise, and the\nmodel\u2019s performance is highly dependent on the quality of the queries. In contrast, PIQN\nleverages learnable vector format queries that dynamically acquire information about\nAppl. Sci. 2024 ,14, 9302 14 of 23\nentities during training. This model enhances flexibility and adaptability across various\nentity types without the need for manual input. Furthermore, the parallel instance query\nmechanism in PIQN allows for the simultaneous prediction of multi-type NEs, capturing\ncorrelations between types and significantly reducing inference time.\n4.3. Other Nested NER Approaches\nThis section introduces two atypical approaches for nested NER: the sequence-to-set\nand the dependency parsing-based approaches. Tan et al. [ 92] and Wu et al. [ 93] focused\non being order-agnostic of NEs and accordingly introduced the sequence-to-set approach.\nTan et al. identify in a single pass by aligning an input sentence with a set of learnable\nentity queries. The self-attention and cross-attention layers are applied for aligning, which\ncapture the relationships between entity queries and between the relationships between the\nsentence and each entity query, respectively. For each query that learns context information\nthrough this process, the model identifies the head and tail indices and class. Wu et al.\nintroduced the propose-and-refine network (PnRNet) that follows a similar process for\naligning a sentence with a set of entity queries. PnRNet utilizes advanced representations of\nentity queries and a sentence. Concretely, the model enumerates all spans from 1 to lgrams\nin a sentence and then selects the top kmost probable ones as entity queries. In addition,\nPnRNet constructs the multi-scale sentence representation by concatenating representations\nof 1 to lgrams. The kentity queries and the multi-scale sentence are processed through\nthe attention layers with a process identical to that used by Tan et al. [ 92]. These sequence-\nto-set approaches take advantage of the fact that NEs are inherently order-independent,\nprocessing multiple entities in a single pass. These models are particularly suitable for\nlearning dependencies among NEs.\nA graph-based dependency parsing aims to analyze the syntactic structure of a sen-\ntence by predicting the head of each token and recognizing its relationship to the head. This\ntask inspired the development of a nested NER model. Yu et al. [ 94] employed a biaffine\nmechanism developed by Dozat and Manning [ 96] for dependency parsing. The biaffine\nmechanism applied to a sentence produces a class score tensor for all word pairs that can\nform a NE, providing a global view of the sentence. However, this mechanism cannot\nconsider additional information, such as the relationships among spans. To consider such\nheterogeneous factors, Yuan et al. [ 95] introduced a triaffine mechanism that advanced\nfrom a biaffine mechanism. Their model uses the triaffine mechanism to fuse various\nfactors such as tokens, boundaries, labels, and related spans. These studies demonstrate\nthat well-designed features and model structures are still helpful for complex tasks such as\nnested NER.\n5. Analysis of Entity Types and Span Representation\nThis section performed three analyses to provide guidance for generating BioNER\nmodels. The first analysis examined the morphological similarities between different\nbiomedical types in practical datasets. Recent studies have built multi-type NER models.\nHowever, biomedical entities often show close similarities, which is reflected in their word\nforms. We observed this phenomenon in practical datasets since it can confuse multi-type\nNER models. Second, we compared the performance of different span representation\nstrategies. The span representation strategy significantly affects performance in nested\nNER models. Therefore, we compiled strategies from the previous studies and compared\ntheir performances. Finally, we evaluated the performance of re-context and decoding\nlayers in PLM-based NER models. Since the advent of PLM-based transfer learning, this\napproach has become the standard for solving various NLP problems, and models that\nutilized PLM with an additional context layer (hereinafter referred to as the re-context layer)\nhave been introduced in several studies. Nevertheless, there has been no study analyzing\nthe performance of the re-context layer in PLM-based models. Therefore, we aimed to\nevaluate the impact of these re-context layers on the NER model. We also conducted\nAppl. Sci. 2024 ,14, 9302 15 of 23\nperformance comparisons among various networks for the decoding layer, which is the\ncomponent that produces the final output.\n5.1. Similarities between Different Entity Types\nBiomedical entities are often closely related across various types, and this characteristic\nis also reflected in word forms. These morphological similarities across different types may\nconfuse the NER model. Therefore, this section observed the morphological similarities\nin practical datasets. For experiments, we integrated the seven widely available datasets:\nBioRED [ 43], JNLPBA [ 46], NCBI disease [ 65], BC2GM [ 54], BC4CHEMD [ 62], BC5CDR [ 51],\nand Linnaeus [66]. All NEs within the datasets underwent a series of preprocessing steps,\nwhich included lemmatization, decapitalization, and the removal of special characters.\nConsequently, their types were normalized into eight classes: gene/protein, DNA, RNA,\ncell, disease, chemical, species, and variant. We defined the similarity of set AtoBwith\nrespect to entity type as follows [97]:\nSimilarity (A,B)=1\n|A|\u2211\na\u2208Amax\nb\u2208BDSC (a,b), (5)\nDSC (a,b)=2|a\u2229b|\n|a|+|b|. (6)\nHere, aandbrepresent word collections of each entity. The similarity between word\ncollections is calculated using the Dice-S\u00f8rensen coefficient (DSC). Note that this similarity\nmeasure is not symmetric for the entity types AandB.\nFigure 4 illustrates the morphological textual similarities between biomedical entity\ntypes, revealing several noteworthy correlations. The strong correlations were observed\namong gene/protein, DNA, RNA, and cell entities. The pair of RNA and gene/protein\nexhibits the highest similarity score of 0.69. Pairs of DNA and gene/protein and RNA and\nDNA also showed high similarity with scores of 0.66 and 0.56, respectively. These outcomes\nare likely due to the functional and structural interconnectivity among them. For example,\nin \u2018myeloid cells (cell)\u2019, the \u2018Mcl-1 (gene/protein)\u2019 is regulated by its promoter \u2018Mcl-1\npromoter (DNA)\u2019, which controls the transcription of \u2018Mcl-1 mRNA (RNA)\u2019. These mor-\nphological similarities can potentially increase the risk of model boundary misclassification\nwhen processing entities of that type simultaneously.\nAppl. Sci. 2024 , 14, x FOR PEER REVIEW  16 of 24 \n \nexample, in \u2018myeloid cells (cell),\u2019 the \u2018Mcl -1 (gene/protein)\u2019 is regulated by its promoter \n\u2018Mcl-1 promoter (DNA),\u2019 which controls the transcription of \u2018Mcl -1 mRNA (RNA).\u2019 These \nmorphological similarities can potentially increase the risk of model boundary  misclassi-\nfication when processing entities of that type simultaneously.  \n \nFigure 4. Morphological textual similarity between biomedical entity types. Strong correlations \nwere observed between gene/protein, DNA, RNA, and cell types exhibiting high biological similar-\nity. The highest similarity is observed between RNA and gene/protein (0.69) , followed by DNA and \ngene/protein (0.66). RNA and DNA also exhibit significant similarity (0.56). Cell correlates notably \nwith gene/protein (0.5) and DNA (0.43).  \nAnother important consideration is that homonymous entities may be classified into \ndifferent types depending on the dataset. For instance, \u2018angiotensin\u2019 is classified as a gene \nor gene product in BioRED and BC2GM  but as a chemical in BC4CHEMD and BC5CDR. \nAdditional examples are provided in Supplementary Table S2. Recently, many studies \nhave focused on integrating multiple datasets to develop multi -type NER models [74 ,76]. \nHowever, our results reveal an inconsistency of type among the datasets, which inevita bly \nhampers model performance. This discrepancy underscores the critical need for standard-\nized annotation guidelines to ensure consistency across datasets.  Additionally, post -pro-\ncessing techniques are necessary to maintain compatibility when integrating different ex-\nisting datasets. Establishing these guidelines and addressing inconsistencies are essential \nfor improving the accuracy and robustness of multi -type NER models.  \n5.2. Strategies for Obtaining Span Representations  \nThe span representation strategy is a key factor in span -based models and signifi-\ncantly impacts model performance. This section evaluated and compared four span rep-\nresentation strategies, such as concatenation, max -pooling, mean -pooling, and PL -marker. \nBecause previous studies have performed comparisons with subsets of these strategies, \nthis experiment aims to provide a comprehensive evaluation of all four strategies.  \nThe concatenation, max -pooling, and mean -pooling strategies generate span repre-\nsentations based on token representations. Formally, given a sentence of \ud835\udc5b tokens, an in-\nput is denoted as \ud835\udc4b={\ud835\udc651,\ud835\udc652,\u2026\ud835\udc65\ud835\udc5b} , and their latent embeddings are denoted as \ud835\udc3b=\n{\u210e\u03021,\u210e\u03022,\u2026,\u210e\u0302\ud835\udc5b} . The candidate spans are defined based on the enumerate method as \n\ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc5b (\ud835\udc4b)={\ud835\udc60\ud835\udc651,\ud835\udc651,\u2026,\ud835\udc60\ud835\udc651,\ud835\udc65\ud835\udc59,\u2026,\ud835\udc60\ud835\udc65\ud835\udc5b\u2212\ud835\udc59+1,\ud835\udc65\ud835\udc5b,\u2026,\ud835\udc60\ud835\udc65\ud835\udc5b,\ud835\udc65\ud835\udc5b} , where \ud835\udc59  is the maximum span length. \nGiven \ud835\udc60\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57, the concatenation  [85], max-pooling, and mean -pooling [98] methods are as \nfollows:  \n\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61 (\ud835\udc60\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57)={\u210e\u0302\ud835\udc56;\u210e\u0302\ud835\udc57}, (7) \nFigure 4. Morphological textual similarity between biomedical entity types. Strong correlations were\nobserved between gene/protein, DNA, RNA, and cell types exhibiting high biological similarity.\nThe highest similarity is observed between RNA and gene/protein (0.69), followed by DNA and\ngene/protein (0.66). RNA and DNA also exhibit significant similarity (0.56). Cell correlates notably\nwith gene/protein (0.5) and DNA (0.43).\nAppl. Sci. 2024 ,14, 9302 16 of 23\nAnother important consideration is that homonymous entities may be classified into\ndifferent types depending on the dataset. For instance, \u2018angiotensin\u2019 is classified as a\ngene or gene product in BioRED and BC2GM but as a chemical in BC4CHEMD and\nBC5CDR. Additional examples are provided in Supplementary Table S2. Recently, many\nstudies have focused on integrating multiple datasets to develop multi-type NER mod-\nels [74,76]. However, our results reveal an inconsistency of type among the datasets, which\ninevitably hampers model performance. This discrepancy underscores the critical need for\nstandardized annotation guidelines to ensure consistency across datasets. Additionally,\npost-processing techniques are necessary to maintain compatibility when integrating dif-\nferent existing datasets. Establishing these guidelines and addressing inconsistencies are\nessential for improving the accuracy and robustness of multi-type NER models.\n5.2. Strategies for Obtaining Span Representations\nThe span representation strategy is a key factor in span-based models and significantly\nimpacts model performance. This section evaluated and compared four span representation\nstrategies, such as concatenation, max-pooling, mean-pooling, and PL-marker. Because pre-\nvious studies have performed comparisons with subsets of these strategies, this experiment\naims to provide a comprehensive evaluation of all four strategies.\nThe concatenation, max-pooling, and mean-pooling strategies generate span repre-\nsentations based on token representations. Formally, given a sentence of ntokens, an\ninput is denoted as X={x1,x2, . . . , xn}, and their latent embeddings are denoted as\nH=n\n\u02c6h1,\u02c6h2, . . . , \u02c6hno\n. The candidate spans are defined based on the enumerate method as\nspan (X)=\b\nsx1,x1, . . . , sx1,xl, . . . , sxn\u2212l+1,xn, . . . , sxn,xn\t\n, where lis the maximum span\nlength. Given sxi,xj, the concatenation [ 85], max-pooling, and mean-pooling [ 98] methods\nare as follows:\nConcat\u0010\nsxi,xj\u0011\n=n\n\u02c6hi;\u02c6hjo\n, (7)\nMax -pooling\u0010\nsxi,xj\u0011\n=n\n\u02c6hi;max\u0010\n\u02c6hi, . . . , \u02c6hj\u0011\n;\u02c6hjo\n, (8)\nMean -pooling\u0010\nsxi,xj\u0011\n=n\n\u02c6hi;mean\u0010\n\u02c6hi, . . . , \u02c6hj\u0011\n;\u02c6hjo\n. (9)\nHere, ; denotes the concatenation operation.\nThe PL-markers refer to individual spans and learn their contextual information [ 87].\nSpecifically, the span candidate sxi,xjis assigned a pair of markers m(s)\nxi,xjandm(e)\nxi,xj, respec-\ntively referring to the start and end tokens of the span. A set of these markers is provided\nas input to the model along with a sentence. The latent embeddings of the marker are then\nemployed as the representation of the span. Consequently, the PL-marker of span sxi,xjis\ndefined as follows [87]:\nPL-Marker\u0010\nsxi,xj\u0011\n=n\n\u02c6h(s)\nmxi,xj;\u02c6h(e)\nmxi,xjo\n. (10)\nThe experiment was performed on three datasets: GENIA [ 69], BB19 [ 70], and Sci-\nERC [ 99]. SciERC is not a biomedical dataset, but it is based on scientific corpora and\ncontains nested NEs. The experiments were conducted on a single NVIDIA Tesla V100\nDGXS 32GB GPU (produced by NVIDIA Corporation in Santa Clara, CA, USA) using\nbioformers/bioformer-16L model. A batch size of 8 and a learning rate of 3 \u00d710\u22125were\napplied. The training was performed for 20 epochs for the GENIA dataset, while the BB19\nand SciERC datasets were trained for 50 epochs. To ensure robustness, each experiment\nwas repeated with five different seeds, and the average precision, recall, and F1-scores were\nused for performance comparison (Table 4). The concatenation strategy achieved an overall\nlower F1-score. Specifically, the differences from best strategies were substantial: \u22120.69\nin GENIA, \u22121.96 in BB19, and \u22122.08 in SciERC. These results demonstrate the effective-\nness of pooling and marker strategies. The strategy that demonstrated consistently high\nperformance was mean-pooling, which achieved the highest F1-score in the BB19 dataset\nAppl. Sci. 2024 ,14, 9302 17 of 23\nand the second highest in both the GENIA and SciERC datasets. On the other hand, the\nPL-Marker strategy showed the best performance in the GENIA and SciERC datasets but\nranked third in the BB19 dataset. These results indicate that the choice of strategy should\nconsider the characteristics of the data. Additionally, although the PL-Marker strategy\nrequires additional computational resources because it allocates two markers per span, it\nproved its worth by outperforming the second-best strategies by 0.58 in GENIA and 0.88 in\nSciERC. Therefore, future work should balance performance and computational cost when\nselecting a span representation strategy.\nTable 4. Comparison of span classification performance according to span representation strategies.\nSpan Repr.\nStrategyGENIA BB19 SciERC\nP R F P R F P R F\nConcatenation 78.07 76.96 77.47 78.31 72.63 75.35 65.78 67.91 66.78\nMax-pooling 78.11 76.73 77.40 79.52 73.80 76.55 67.57 68.09 67.81\nMean-pooling 78.84 76.37 77.58 79.92 74.89 77.31 67.52 68.48 67.98\nPL-Marker 78.34 78.01 78.16 79.17 73.25 76.09 67.53 70.27 68.86\nBold indicates the best scores, and underline indicates second-best scores.\n5.3. Contribution of Re-Context and Decoding Layers\nMost recent NLP models adopt transfer learning based on PLM. For instance, as\nillustrated in Figure 2a, the PLM is applied to both the text vectorization and context\nlayer. Several studies implement a re-context layer on top of PLM to capture additional\ncontextual information. This layer may include a recurrent neural network (RNN) or CNN.\nLastly, the decoding layer generates corresponding labels for downstream tasks based on\nthe representation.\nIn this section, we introduced the widely used re-context and decoding layers in flat\nNER models and analyzed their individual impacts. The experiments were performed on\nthree datasets: NCBI disease [ 65], BC2GM [ 54], and BC5CDR chemical [ 51]. Each model\nwas trained and tested on five seeds, and the averaged precision, recall, and F1-score\nwere used for comparison. The bold numbers indicate the best scores, and the underlined\nnumbers represent the second-best scores. All experiments utilized a single NVIDIA Tesla\nV100 DGXS 32GB GPU for training the NER models. The bioformers/bioformer-16L model\nwas used as the PLM. A batch size of 32 and a learning rate of 3 \u00d710\u22125were employed.\nThe training was performed for between 20 and 30 epochs, with early stopping triggered\nafter 5 consecutive epochs without improvement.\n5.3.1. Comparison of Re-Context Layers\nRe-context layers have been employed to obtain additional contextual information;\nhowever, to the best of our knowledge, their effectiveness has not been comprehensively\nevaluated since the advent of PLMs. This section evaluates the effects of six re-context\nlayers on NER tasks: Bi-LSTM, 1-dimension CNN (1dCNN), 2-dimension CNN (2dCNN),\nBiLSTM-attention (BiLSTM-attn), 1dCNN-attn, and 2dCNN-attn. Bi-LSTM, 1dCNN, and\nattention mechanisms have traditionally been applied to text data, whereas 2dCNN has\nprimarily been employed in image-related tasks due to its ability to capture local patterns\nand features. However, since Banerjee et al. suggested that the localized perspective of\n2dCNN can enhance the performance of NER tasks [ 75], our NER experiments included it\nas a re-context layer.\nIn our experimental setup, the input dimensions were in the shape of (sentence length,\nembedding size). The 1dCNN layer had a kernel size of 5, a padding size of 2, and an\noutput channel size double that of the input size. The 2dCNN layer was designed with a\nkernel size of (5, 5), a stride size of (1, 2), and circular padding of (2, 256). The settings for\nthe 2dCNN layer were referenced in a previous study [ 75]. The decoding layer utilized a\nsoftmax layer for sequence labeling, following the IOB tagging format.\nAppl. Sci. 2024 ,14, 9302 18 of 23\nTable 5 compares the performance of six models with re-context layers to the base\nmodel, which is a model without any re-context layer. The majority of models with the re-\ncontext layer generally outperformed the base model (except for 2dCNN and BiLSTM-attn\non BC5CDR chemical); however, the differences from the base model were not significant.\nSpecifically, the differences from the best model were \u22120.43 in NCBI disease, \u22120.44 in\nBC2GM, and \u22120.39 in BC5CDR chemical. We also observed that the performance dif-\nferences between re-context layers were not substantial. These results suggest that the\nPLM already contains sufficient contextual information for NER tasks, so the additional\nre-context layers may offer only marginal improvements. Therefore, the focus should be on\noptimizing other components of the NER to achieve better performance.\nTable 5. Comparison of the flat NER performance according to the re-contextualization layer.\nRe-Context\nLayerNCBI Disease BC2GM BC5CDR Chemical\nP R F P R F P R F\nBase model 86.14 89.10 87.60 82.85 84.08 83.46 92.36 93.46 92.90\n1dCNN 86.93 89.08 87.99 82.98 84.14 83.55 92.67 93.89 93.28\n2dCNN 86.70 88.94 87.80 83.19 84.44 83.81 92.42 93.29 92.86\nBiLSTM 86.91 89.19 88.03 83.05 84.17 83.60 92.72 93.38 93.05\n1dCNN-attn 86.73 89.31 88.00 83.02 84.64 83.82 92.99 93.28 93.13\n2dCNN-attn 86.33 88.96 87.63 83.02 84.30 83.65 92.84 93.75 93.29\nBiLSTM-attn 86.52 89.21 87.84 83.43 84.37 83.90 92.41 93.28 92.84\nBold indicates the best scores, and underline indicates second-best scores.\n5.3.2. Comparison of Decoder Layers\nIn this section, we trained and evaluated four decoding layers frequently used in NER\nmodels: softmax , CRF, RNN, and span labeling. Softmax , CRF, and RNN are employed for\nthe sequence labeling approach, which is common for flat NER tasks. On the other hand,\nthe span labeling layer is not as commonly used for flat NER. Nonetheless, we included it\nin the experiment to observe the adaptability of the span labeling approach to flat NER.\nSoftmax is favored for its computational efficiency and straightforward implementation.\nIt is used to assign probabilities to each class for individual tokens independently. On the\nother hand, CRF and RNN capture the dependencies between tokens. CRF learns transition\nprobabilities between adjacent tokens, capturing that certain tags are likelier to follow\nothers. RNN maintains information about previous tokens when predicting the tag of\nthe current token, capturing sequential dependencies. Both CRF and RNN require higher\ncomputational costs than softmax . On the other hand, the span labeling layer was adapted\nfor flat NER tasks in this experiment. The model with a span labeling layer enumerated\ncandidate spans of up to 8 words, generating span representations using a concatenation\nstrategy. The model then classified these candidate spans based on their representations.\nTo ensure a flat structure, nested entities were consolidated by prioritizing the later and\nlonger spans that are classified as entities.\nAccording to Table 6, the RNN and CRF exhibited high overall performance. Specifi-\ncally, RNN performed best on the NCBI disease and BC5CDR chemical datasets, and CRF\nsecured the second-best performance across all datasets. This indicates that dependency\ninformation between tokens is valuable in the NER model. Notably, the span labeling\napproach showed low performance on the NCBI disease and BC5CDR chemical datasets,\nachieving high performance only on BC2GM. There is even a +0.86% difference from\nsoftmax . These results are intriguing, considering the simplicity of the post-process of\nhandling nested entities. However, it should be noted that the span labeling layer incurs a\nhigher computational cost than CRF and RNN due to the generation and classification of\nnumerous candidate spans. Therefore, CRF and RNN can be partial and effective choices\nwhen considering both computational cost and performance.\nAppl. Sci. 2024 ,14, 9302 19 of 23\nTable 6. Comparison of the flat NER performance according to the decoding layer.\nDecoding\nLayerNCBI Disease BC2GM BC5CDR Chemical\nP R F P R F P R F\nSoftMax 86.14 89.10 87.60 82.85 84.08 83.46 92.36 93.46 92.90\nCRF 86.04 89.27 87.62 83.49 84.77 84.13 92.74 93.68 93.20\nRNN 87.32 89.40 88.35 83.61 84.45 84.02 92.49 93.96 93.22\nSpan labeling 87.06 87.48 87.27 85.36 83.31 84.32 93.38 92.43 92.91\nBold indicates the best scores, and underline indicates second-best scores.\n6. Discussion\nAfter the emergence of LLMs, BioNERs have shown significant improvements, with\nflat NER models achieving around 90% accuracy for most datasets and nested NER models\nachieving around 80%. However, there are still considerable limitations that hinder their\napplication in real-world environments, particularly in terms of the lack of diverse datasets\nand generalization across datasets.\nFirst, lack of data remains a key issue. The scarcity of domain-specific datasets,\nespecially for nested NER tasks, presents a significant challenge. There are only two\navailable corpora for biomedical nested NER, GENIA and BB19, which hinder generalizing\nthe models across different data. While flat NER has more datasets, there is a lack of multi-\ntype corpora. Manually creating large-scale datasets with annotation is labor-intensive and\nexpensive. To overcome these challenges, data augmentation and meta-learning models are\nactively researched. Data augmentation systems attempt to generate synthetic examples\nor transform existing data, while the meta-learning models enable effective performance\nwith limited labeled examples. Despite the progress in these areas, the performance\nof these methods has not yet reached a sufficient level for practical use in real-world\napplications [100\u2013102].\nSecondly, generalization across datasets presents another challenge. A model that per-\nforms well on one dataset often sees its performance decline when applied to other datasets\nwithin the same domain (e.g., NCBI disease to BC5CDR disease) [ 71]. This decline is often\ndue to differences in annotation schemes, boundary definitions, and entity types between\ndatasets. Models that struggle to maintain consistent performance across multiple datasets\nare less reliable for real-world applications. To address this, there is a need for models that\ncan generalize effectively across various datasets and handle these discrepancies.\n7. Conclusions\nThis review comprehensively analyzes flat and nested NER tasks in the biomedical\ndomain, underscoring recent advancements and persisting challenges. While LLMs have\nmarkedly improved the performance of flat NER tasks, substantial challenges remain to\naddress the complexities of nested NER. Our analysis highlighted the limitations of nested\nentity processing strategies that require excessive resources and utilize span representation\nmethods that lack contextual considerations. Additionally, the limited availability of\nannotated datasets containing diverse entity types is a significant bottleneck for scaling\nNER tasks. Furthermore, annotation inconsistencies across different corpora hinder models\nfrom achieving reliable performance on datasets of similar types. Future research efforts\nshould focus on developing robust models that can maintain consistent performance across\ndifferent datasets and achieve high-quality learning with limited resources. Addressing\nthese challenges is essential for building reliable BioNER systems that can be effectively\nused in real-world biomedical applications.\nSupplementary Materials: The following supporting information can be downloaded at: https:\n//www.mdpi.com/article/10.3390/app14209302/s1, Table S1: Performance evaluation of tradi-\ntional approaches and LLM-based models on the flat NER datasets; Table S2: Examples of the type\ndiscrepancy among biomedical datasets.\nAppl. Sci. 2024 ,14, 9302 20 of 23\nAuthor Contributions: Conceptualization, Y.P ., G.S. and M.R.; investigation, Y.P . and G.S.;\nwritin g\u2014o riginal draft preparation, Y.P . and G.S.; writing\u2014review and editing, Y.P . and M.R.; super-\nvision, M.R.; funding acquisition, M.R. All authors have read and agreed to the published version of\nthe manuscript.\nFunding: This research was partly supported by Korea Institute of Marine Science & Technology\nPromotion (KIMST) funded by the Ministry of Oceans and Fisheries, Korea (20220517) and Institute\nof Information & communications Technology Planning & Evaluation (IITP) grant funded by the\nKorea government (MSIT) (No. RS-2020-II201373, Artificial Intelligence Graduate School Program\n(Hanyang University)).\nConflicts of Interest: The authors declare no conflict of interest.\nReferences\n1. Yamada, K.; Miwa, M.; Sasaki, Y. Biomedical Relation Extraction with Entity Type Markers and Relation-specific Question\nAnswering. In Proceedings of the 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks,\nToronto, ON, Canada, 13\u201314 July 2023; pp. 377\u2013384.\n2. Frisoni, G.; Italiani, P .; Moro, G.; Bartolini, I.; Boschetti, M.A.; Carbonaro, A. Graph-Enhanced Biomedical Abstractive Summariza-\ntion via Factual Evidence Extraction. SN Comput. Sci. 2023 ,4, 500. [CrossRef]\n3. Lai, P .-T.; Wei, C.-H.; Luo, L.; Chen, Q.; Lu, Z. BioREx: Improving biomedical relation extraction by leveraging heterogeneous\ndatasets. J. Biomed. Inform. 2023 ,146, 104487. [CrossRef] [PubMed]\n4. Al-Moslmi, T.; Oca\u00f1a, M.G.; Opdahl, A.L.; Veres, C. Named entity extraction for knowledge graphs: A literature overview. IEEE\nAccess 2020 ,8, 32862\u201332881. [CrossRef]\n5. Yang, S.; Yoo, S.; Jeong, O. DeNERT-KG: Named entity and relation extraction model using DQN, knowledge graph, and BERT.\nAppl. Sci. 2020 ,10, 6429. [CrossRef]\n6. Park, Y.; Lee, J.; Moon, H.; Choi, Y.S.; Rho, M. Discovering microbe-disease associations from the literature using a hierarchical\nlong short-term memory network and an ensemble parser model. Sci. Rep. 2021 ,11, 4490. [CrossRef]\n7. Grishman, R.; Sundheim, B.M. Message understanding conference-6: A brief history. In Proceedings of the COLING 1996 Volume\n1: The 16th International Conference on Computational Linguistics, Copenhagen, Denmark, 5\u20139 August 1996.\n8. Li, J.; Sun, A.; Han, J.; Li, C. A survey on deep learning for named entity recognition. IEEE Trans. Knowl. Data Eng. 2020 ,34, 50\u201370.\n[CrossRef]\n9. Campos, D.; Matos, S.; Oliveira, J.L. Biomedical named entity recognition: A survey of machine-learning tools. Theory Appl. Adv.\nText Min. 2012 ,11, 175\u2013195.\n10. Wang, X.; Yang, C.; Guan, R. A comparative study for biomedical named entity recognition. Int. J. Mach. Learn. Cybern. 2018 ,9,\n373\u2013382. [CrossRef]\n11. Song, B.; Li, F.; Liu, Y.; Zeng, X. Deep learning methods for biomedical named entity recognition: A survey and qualitative\ncomparison. Brief. Bioinform. 2021 ,22, bbab282. [CrossRef]\n12. Gaizauskas, R. Term recognition and classification in biological science journal articles. In Proceedings of the Workshop on\nComputational Terminology for Medical and Biological Applications, Patras, Greece, 2\u20134 June 2000.\n13. Song, M.; Yu, H.; Han, W.-S. Developing a hybrid dictionary-based bio-entity recognition technique. BMC Med. Inform. Decis.\nMak. 2015 ,15, 1\u20138. [CrossRef]\n14. Zhou, G. Recognizing names in biomedical texts using mutual information independence model and SVM plus sigmoid. Int. J.\nMed. Inform. 2006 ,75, 456\u2013467. [CrossRef] [PubMed]\n15. Alex, B.; Haddow, B.; Grover, C. Recognising nested named entities in biomedical text. In Proceedings of the Biological,\nTranslational, and Clinical Language Processing, Prague, Czech Republic, 29 June 2007; pp. 65\u201372.\n16. Leaman, R.; Wei, C.-H.; Lu, Z. tmChem: A high performance approach for chemical named entity recognition and normalization.\nJ. Cheminform. 2015 ,7, S3. [CrossRef] [PubMed]\n17. Gridach, M. Character-level neural network for biomedical named entity recognition. J. Biomed. Inform. 2017 ,70, 85\u201391. [CrossRef]\n[PubMed]\n18. Yoon, W.; So, C.H.; Lee, J.; Kang, J. Collabonet: Collaboration of deep neural networks for biomedical named entity recognition.\nBMC Bioinform. 2019 ,20, 55\u201365. [CrossRef]\n19. Cho, H.; Lee, H. Biomedical named entity recognition using deep neural networks with contextual information. BMC Bioinform.\n2019 ,20, 735. [CrossRef]\n20. Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding.\narXiv 2018 , arXiv:1810.04805.\n21. Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.R.; Le, Q.V . Xlnet: Generalized autoregressive pretraining for language\nunderstanding. arXiv 2019 , arXiv:1906.08237.\n22. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.D.; Dhariwal, P .; Neelakantan, A.; Shyam, P .; Sastry, G.; Askell, A. Language\nmodels are few-shot learners. Adv. Neural Inf. Process. Syst. 2020 ,33, 1877\u20131901.\nAppl. Sci. 2024 ,14, 9302 21 of 23\n23. Lee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C.H.; Kang, J. BioBERT: A pre-trained biomedical language representation model\nfor biomedical text mining. Bioinformatics 2020 ,36, 1234\u20131240. [CrossRef]\n24. Naseem, U.; Musial, K.; Eklund, P .; Prasad, M. Biomedical named-entity recognition by hierarchically fusing biobert representa-\ntions and deep contextual-level word-embedding. In Proceedings of the 2020 International Joint Conference on Neural Networks\n(IJCNN), Glasgow, UK, 19\u201324 July 2020; pp. 1\u20138.\n25. Beltagy, I.; Peters, M.E.; Cohan, A. Longformer: The long-document transformer. arXiv 2020 , arXiv:2004.05150.\n26. Sanh, V . DistilBERT, A Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter. arXiv 2019 , arXiv:1910.01108.\n27. Rohanian, O.; Nouriborji, M.; Kouchaki, S.; Clifton, D.A. On the effectiveness of compact biomedical transformers. Bioinformatics\n2023 ,39, btad103. [CrossRef] [PubMed]\n28. Sang, E.F.; De Meulder, F. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. arXiv\n2003 , arXiv:cs/0306050.\n29. Ratinov, L.; Roth, D. Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth\nConference on Computational Natural Language Learning (CoNLL-2009), Boulder, CO, USA, 4\u20135 June 2009; pp. 147\u2013155.\n30. Ramshaw, L.A.; Marcus, M.P . Text chunking using transformation-based learning. In Natural Language Processing Using Very Large\nCorpora ; Springer: Berlin/Heidelberg, Germany, 1999; pp. 157\u2013176.\n31. Lample, G. Neural architectures for named entity recognition. arXiv 2016 , arXiv:1603.01360.\n32. Finkel, J.R.; Manning, C.D. Nested named entity recognition. In Proceedings of the 2009 Conference on Empirical Methods in\nNatural Language Processing, Singapore, 6\u20137 August 2009; pp. 141\u2013150.\n33. Wang, Y.; Tong, H.; Zhu, Z.; Li, Y. Nested named entity recognition: A survey. ACM Trans. Knowl. Discov. Data (TKDD) 2022 ,16,\n108. [CrossRef]\n34. Olson, D.L.; Delen, D. Advanced Data Mining Techniques ; Springer Science & Business Media: Berlin/Heidelberg, Germany, 2008.\n35. Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G.S.; Dean, J. Distributed representations of words and phrases and their compo-\nsitionality. In Proceedings of the Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural\nInformation Processing Systems 2013, Lake Tahoe, NV , USA, 5\u201310 December 2013.\n36. Pennington, J.; Socher, R.; Manning, C.D. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, 25\u201329 October 2014; pp. 1532\u20131543.\n37. Bojanowski, P .; Grave, E.; Joulin, A.; Mikolov, T. Enriching word vectors with subword information. Trans. Assoc. Comput. Linguist.\n2017 ,5, 135\u2013146. [CrossRef]\n38. Chen, X.; Xu, L.; Liu, Z.; Sun, M.; Luan, H. Joint learning of character and word embeddings. In Proceedings of the Twenty-Fourth\nInternational Joint Conference on Artificial Intelligence, Buenos Aires, Argentina, 25\u201331 July 2015.\n39. Peters, M.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. Deep contextualized word representations.\narXiv 2018 , arXiv:1802.05365.\n40. Huang, K.; Altosaar, J.; Ranganath, R. Clinicalbert: Modeling clinical notes and predicting hospital readmission. arXiv 2019 ,\narXiv:1904.05342.\n41. Gu, Y.; Tinn, R.; Cheng, H.; Lucas, M.; Usuyama, N.; Liu, X.; Naumann, T.; Gao, J.; Poon, H. Domain-specific language model\npretraining for biomedical natural language processing. ACM Trans. Comput. Healthc. (HEALTH) 2021 ,3, 2. [CrossRef]\n42. Fang, L.; Chen, Q.; Wei, C.-H.; Lu, Z.; Wang, K. Bioformer: An efficient transformer language model for biomedical text mining.\narXiv 2023 , arXiv:2302.01588.\n43. Luo, L.; Lai, P .T.; Wei, C.H.; Arighi, C.N.; Lu, Z. BioRED: A rich biomedical relation extraction dataset. Brief. Bioinform. 2022 ,23,\nbbac282. [CrossRef] [PubMed]\n44. Mohan, S.; Li, D. Medmentions: A large biomedical corpus annotated with umls concepts. arXiv 2019 , arXiv:1902.09476.\n45. Bada, M.; Eckert, M.; Evans, D.; Garcia, K.; Shipley, K.; Sitnikov, D.; Baumgartner, W.A., Jr.; Cohen, K.B.; Verspoor, K.; Blake, J.A.;\net al. Concept annotation in the CRAFT corpus. BMC Bioinform. 2012 ,13, 161. [CrossRef] [PubMed]\n46. Kim, J.-D.; Ohta, T.; Tsuruoka, Y.; Tateisi, Y.; Collier, N. Introduction to the bio-entity recognition task at JNLPBA. In Proceedings\nof the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications, Geneva, Switzerland,\n28\u201329 August 2004; pp. 70\u201375.\n47. Neves, M.; Damaschun, A.; Kurtz, A.; Leser, U. Annotating and evaluating text for stem cell research. In Proceedings of the\nThird Workshop on Building and Evaluation Resources for Biomedical Text Mining (BioTxtM 2012) at Language Resources and\nEvaluation (LREC), Istanbul, Turkey, 21\u201327 May 2012; pp. 16\u201323.\n48. Bagewadi, S.; Bobi\u00b4 c, T.; Hofmann-Apitius, M.; Fluck, J.; Klinger, R. Detecting miRNA mentions and relations in biomedical\nliterature. F1000Research 2014 ,3, 205. [CrossRef] [PubMed]\n49. Nagel, K.; Jimeno-Yepes, A.; Rebholz-Schuhmann, D. Annotation of protein residues based on a literature analysis: Cross-\nvalidation against UniProtKb. BMC Bioinform. 2009 ,10, S4. [CrossRef]\n50. Thompson, P .; Iqbal, S.A.; McNaught, J.; Ananiadou, S. Construction of an annotated corpus to support biomedical information\nextraction. BMC Bioinform. 2009 ,10, 349. [CrossRef]\n51. Li, J.; Sun, Y.; Johnson, R.J.; Sciaky, D.; Wei, C.-H.; Leaman, R.; Davis, A.P .; Mattingly, C.J.; Wiegers, T.C.; Lu, Z. BioCreative V\nCDR task corpus: A resource for chemical disease relation extraction. Database 2016 ,2016 , baw068. [CrossRef]\nAppl. Sci. 2024 ,14, 9302 22 of 23\n52. Miranda, A.; Mehryary, F.; Luoma, J.; Pyysalo, S.; Valencia, A.; Krallinger, M. Overview of DrugProt BioCreative VII track: Quality\nevaluation and large scale text mining of drug-gene/protein relations. In Proceedings of the Seventh BioCreative Challenge\nEvaluation Workshop, Virtual Event, 8\u201310 November 2021; pp. 11\u201321.\n53. Wei, C.-H.; Allot, A.; Riehle, K.; Milosavljevic, A.; Lu, Z. tmVar 3.0: An improved variant concept recognition and normalization\ntool. Bioinformatics 2022 ,38, 4449\u20134451. [CrossRef]\n54. Smith, L.; Tanabe, L.K.; Kuo, C.-J.; Chung, I.; Hsu, C.-N.; Lin, Y.-S.; Klinger, R.; Friedrich, C.M.; Ganchev, K.; Torii, M. Overview of\nBioCreative II gene mention recognition. Genome Biol. 2008 ,9, S2. [CrossRef]\n55. Wang, X.; Tsujii, J.i.; Ananiadou, S. Disambiguating the species of biomedical named entities using natural language parsers.\nBioinformatics 2010 ,26, 661\u2013667. [CrossRef]\n56. Gerner, M.; Nenadic, G.; Bergman, C.M. An exploration of mining gene expression mentions and their anatomical locations\nfrom biomedical text. In Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, Uppsala, Sweden,\n15 July 2010; pp. 72\u201380.\n57. Cejuela, J.M.; Vinchurkar, S.; Goldberg, T.; Prabhu Shankar, M.S.; Baghudana, A.; Bojchevski, A.; Uhlig, C.; Ofner, A.; Raharja-Liu,\nP .; Jensen, L.J. LocText: Relation extraction of protein localizations to assist database curation. BMC Bioinform. 2018 ,19, 15.\n[CrossRef] [PubMed]\n58. Faessler, E.; Modersohn, L.; Lohr, C.; Hahn, U. ProGene\u2014A large-scale, high-quality protein-gene annotated benchmark corpus.\nIn Proceedings of the 12th Language Resources and Evaluation Conference, Marseille, France, 11\u201316 May 2020; pp. 4585\u20134596.\n59. Islamaj, R.; Wei, C.-H.; Cissel, D.; Miliaras, N.; Printseva, O.; Rodionov, O.; Sekiya, K.; Ward, J.; Lu, Z. NLM-Gene, a richly\nannotated gold standard dataset for gene entities that addresses ambiguity and multi-species gene recognition. J. Biomed. Inform.\n2021 ,118, 103779. [CrossRef] [PubMed]\n60. Kol \u00e1rik, C.; Klinger, R.; Friedrich, C.M.; Hofmann-Apitius, M.; Fluck, J. Chemical names: Terminological resources and corpora\nannotation. In Proceedings of the Workshop on Building and Evaluating Resources for Biomedical Text Mining (6th Edition of\nthe Language Resources and Evaluation Conference), Osaka, Japan, 11\u201316 December 2008.\n61. Herrero-Zazo, M.; Segura-Bedmar, I.; Mart \u00ednez, P .; Declerck, T. The DDI corpus: An annotated corpus with pharmacological\nsubstances and drug\u2013drug interactions. J. Biomed. Inform. 2013 ,46, 914\u2013920. [CrossRef]\n62. Krallinger, M.; Rabal, O.; Leitner, F.; Vazquez, M.; Salgado, D.; Lu, Z.; Leaman, R.; Lu, Y.; Ji, D.; Lowe, D.M. The CHEMDNER\ncorpus of chemicals and drugs and its annotation principles. J. Cheminform. 2015 ,7, S2. [CrossRef] [PubMed]\n63. Islamaj, R.; Leaman, R.; Kim, S.; Kwon, D.; Wei, C.-H.; Comeau, D.C.; Peng, Y.; Cissel, D.; Coss, C.; Fisher, C. NLM-Chem, a new\nresource for chemical entity recognition in PubMed full text literature. Sci. Data 2021 ,8, 91. [CrossRef] [PubMed]\n64. Gurulingappa, H.; Klinger, R.; Hofmann-Apitius, M.; Fluck, J. An empirical evaluation of resources for the identification of\ndiseases and adverse effects in biomedical literature. In Proceedings of the 2nd Workshop on Building and Evaluating Resources\nfor Biomedical Text Mining (7th Edition of the Language Resources and Evaluation Conference), Valetta, Malta, 18 March 2010;\npp. 15\u201322.\n65. Do\u02d8 gan, R.I.; Leaman, R.; Lu, Z. NCBI disease corpus: A resource for disease name recognition and concept normalization. J.\nBiomed. Inform. 2014 ,47, 1\u201310. [CrossRef] [PubMed]\n66. Gerner, M.; Nenadic, G.; Bergman, C.M. LINNAEUS: A species name identification system for biomedical literature. BMC\nBioinform. 2010 ,11, 85. [CrossRef] [PubMed]\n67. Luoma, J.; Nastou, K.; Ohta, T.; Toivonen, H.; Pafilis, E.; Jensen, L.J.; Pyysalo, S. S1000: A better taxonomic name corpus for\nbiomedical information extraction. Bioinformatics 2023 ,39, btad369. [CrossRef]\n68. Thomas, P .E.; Klinger, R.; Furlong, L.I.; Hofmann-Apitius, M.; Friedrich, C.M. Challenges in the association of human single\nnucleotide polymorphism mentions with unique database identifiers. BMC Bioinform. 2011 ,12, S4. [CrossRef]\n69. Kim, J.-D.; Ohta, T.; Tateisi, Y.; Tsujii, J.i. GENIA corpus\u2014A semantically annotated corpus for bio-textmining. Bioinformatics 2003 ,\n19, i180\u2013i182. [CrossRef]\n70. Bossy, R.; Del \u00e9ger, L.; Chaix, E.; Ba, M.; N \u00e9dellec, C. Bacteria biotope at BioNLP open shared tasks 2019. In Proceedings of the 5th\nWorkshop on BioNLP Open Shared Tasks, Hong Kong, China, 4 November 2019; pp. 121\u2013131.\n71. K\u00fchnel, L.; Fluck, J. We are not ready yet: Limitations of state-of-the-art disease named entity recognizers. J. Biomed. Semant. 2022 ,\n13, 26. [CrossRef] [PubMed]\n72. Sun, C.; Yang, Z.; Wang, L.; Zhang, Y.; Lin, H.; Wang, J. Biomedical named entity recognition using BERT in the machine reading\ncomprehension framework. J. Biomed. Inform. 2021 ,118, 103799. [CrossRef] [PubMed]\n73. Khan, M.R.; Ziyadi, M.; AbdelHady, M. Mt-bioner: Multi-task learning for biomedical named entity recognition using deep\nbidirectional transformers. arXiv 2020 , arXiv:2001.08904.\n74. Chai, Z.; Jin, H.; Shi, S.; Zhan, S.; Zhuo, L.; Yang, Y. Hierarchical shared transfer learning for biomedical named entity recognition.\nBMC Bioinform. 2022 ,23, 8. [CrossRef] [PubMed]\n75. Banerjee, P .; Pal, K.K.; Devarakonda, M.; Baral, C. Biomedical named entity recognition via knowledge guidance and question\nanswering. ACM Trans. Comput. Healthc. 2021 ,2, 33. [CrossRef]\n76. Luo, L.; Wei, C.-H.; Lai, P .-T.; Leaman, R.; Chen, Q.; Lu, Z. AIONER: All-in-one scheme-based biomedical named entity recognition\nusing deep learning. Bioinformatics 2023 ,39, btad310. [CrossRef]\n77. Moscato, V .; Postiglione, M.; Sansone, C.; Sperli, G. Taughtnet: Learning multi-task biomedical named entity recognition from\nsingle-task teachers. IEEE J. Biomed. Health Inform. 2023 ,27, 2512\u20132523. [CrossRef]\nAppl. Sci. 2024 ,14, 9302 23 of 23\n78. Moen, S.; Ananiadou, T.S.S. Distributional semantics resources for biomedical text processing. In Proceedings of the LBM 2013,\nTokyo, Japan, 12\u201313 December 2013; pp. 39\u201344.\n79. Jin, Q.; Dhingra, B.; Cohen, W.W.; Lu, X. Probing biomedical embeddings from language models. arXiv 2019 , arXiv:1904.02181.\n80. Doddington, G.R.; Mitchell, A.; Przybocki, M.A.; Ramshaw, L.A.; Strassel, S.M.; Weischedel, R.M. The automatic content extraction\n(ace) program-tasks, data, and evaluation. In Proceedings of the LREC, Lisbon, Portugal, 26\u201328 May 2004; pp. 837\u2013840.\n81. Walker, C.; Consortium, L.D. ACE 2005 Multilingual Training Corpus ; Linguistic Data Consortium: Philadelphia, PA, USA, 2005.\n82. Getman, J.; Ellis, J.; Song, Z.; Tracey, J.; Strassel, S.M. Overview of Linguistic Resources for the TAC KBP 2017 Evaluations:\nMethodologies and Results. In Proceedings of the TAC, Gaithersburg, MD, USA, 13\u201314 November 2017.\n83. Fisher, J.; Vlachos, A. Merge and Label: A novel neural network architecture for nested NER. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics (ACL 2019), Florence, Italy, 28 July\u20132 August 2019; pp. 5840\u20135850.\n84. Wang, J.; Shou, L.; Chen, K.; Chen, G. Pyramid: A layered model for nested named entity recognition. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, Online, 5\u201310 July 2020; pp. 5918\u20135928.\n85. Zhong, Z.; Chen, D. A Frustratingly Easy Approach for Entity and Relation Extraction. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online, 6\u201311\nJune 2021; pp. 50\u201361.\n86. Zuo, M.; Zhang, Y. A span-based joint model for extracting entities and relations of bacteria biotopes. Bioinformatics 2022 ,38,\n220\u2013227. [CrossRef]\n87. Ye, D.; Lin, Y.; Li, P .; Sun, M. Packed Levitated Marker for Entity and Relation Extraction. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, 22\u201327 May 2022; pp.\n4904\u20134917.\n88. Tan, C.A.Q.; Qiu, W.; Chen, M.S.; Wang, R.; Huang, F. Boundary Enhanced Neural Span Classification for Nested Named Entity\nRecognition. AAAI Conf. Artif. Intell. 2020 ,34, 9016\u20139023. [CrossRef]\n89. Shen, Y.; Ma, X.; Tan, Z.; Zhang, S.; Wang, W.; Lu, W. Locate and Label: A Two-stage Identifier for Nested Named Entity\nRecognition. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), Virtual Event, 1\u20136 August 2021; pp.\n2782\u20132794.\n90. Li, X.; Feng, J.; Meng, Y.; Han, Q.; Wu, F.; Li, J. A Unified MRC Framework for Named Entity Recognition. In Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics, Online, 5\u201310 July 2020; pp. 5849\u20135859.\n91. Shen, Y.; Wang, X.; Tan, Z.; Xu, G.; Xie, P .; Huang, F.; Lu, W.; Zhuang, Y. Parallel Instance Query Network for Named Entity\nRecognition. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), Dublin, Ireland, 22\u201327 May 2022; pp. 947\u2013961.\n92. Tan, Z.; Shen, Y.; Zhang, S.; Lu, W.; Zhuang, Y. A sequence-to-set network for nested named entity recognition. arXiv 2021 ,\narXiv:2105.08901.\n93. Wu, S.; Shen, Y.; Tan, Z.; Lu, W. Propose-and-refine: A two-stage set prediction network for nested named entity recognition.\narXiv 2022 , arXiv:2204.12732.\n94. Yu, J.; Bohnet, B.; Poesio, M. Named Entity Recognition as Dependency Parsing. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, Online, 5\u201310 July 2020; pp. 6470\u20136476.\n95. Yuan, Z.; Tan, C.; Huang, S.; Huang, F. Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity\nRecognition. In Proceedings of the Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, 22\u201327\nMay 2022; pp. 3174\u20133186.\n96. Dozat, T.; Manning, C.D. Deep biaffine attention for neural dependency parsing. arXiv 2016 , arXiv:1611.01734.\n97. Dice, L.R. Measures of the amount of ecologic association between species. Ecology 1945 ,26, 297\u2013302. [CrossRef]\n98. Yu, J.; Ji, B.; Li, S.; Ma, J.; Liu, H.; Xu, H. S-NER: A Concise and Efficient Span-Based Model for Named Entity Recognition. Sensors\n2022 ,22, 2852. [CrossRef]\n99. Luan, Y.; He, L.; Ostendorf, M.; Hajishirzi, H. Multi-task identification of entities, relations, and coreference for scientific\nknowledge graph construction. arXiv 2018 , arXiv:1808.09602.\n100. Chen, P .; Wang, J.; Lin, H.; Zhao, D.; Yang, Z. Few-shot biomedical named entity recognition via knowledge-guided instance\ngeneration and prompt contrastive learning. Bioinformatics 2023 ,39, btad496. [CrossRef]\n101. Zhou, R.; Li, X.; He, R.; Bing, L.; Cambria, E.; Si, L.; Miao, C. MELM: Data augmentation with masked entity language modeling\nfor low-resource NER. arXiv 2021 , arXiv:2108.13655.\n102. Wang, S.; Sun, X.; Li, X.; Ouyang, R.; Wu, F.; Zhang, T.; Li, J.; Wang, G. Gpt-ner: Named entity recognition via large language\nmodels. arXiv 2023 , arXiv:2304.10428.\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.",
  "keywords": [
    "identifiers",
    "bioinformatics",
    "identifier",
    "biomedical",
    "entities",
    "entity",
    "biomed",
    "abbreviations",
    "annotation",
    "annotating"
  ],
  "intent_category": "named_entity_recognition",
  "named_entities": [
    {
      "text": " Park, Y",
      "label": "PER"
    },
    {
      "text": " Son, G",
      "label": "PER"
    },
    {
      "text": " Rho, M",
      "label": "PER"
    },
    {
      "text": " Appl.",
      "label": "ORG"
    },
    {
      "text": " Lykourgos\nMagafas",
      "label": "PER"
    },
    {
      "text": " Rui Ara \u00fajo",
      "label": "PER"
    },
    {
      "text": " MDPI",
      "label": "ORG"
    },
    {
      "text": " Basel",
      "label": "LOC"
    },
    {
      "text": " Switzerland",
      "label": "LOC"
    },
    {
      "text": " Creative Commons\nAttribution",
      "label": "MISC"
    },
    {
      "text": "CC BY",
      "label": "MISC"
    },
    {
      "text": "Yesol Park",
      "label": "PER"
    },
    {
      "text": "\n",
      "label": "PER"
    },
    {
      "text": " Gyujin Son",
      "label": "PER"
    },
    {
      "text": " Mina Rho",
      "label": "PER"
    },
    {
      "text": "Department of Computer Science",
      "label": "ORG"
    },
    {
      "text": " Hanyang University",
      "label": "ORG"
    },
    {
      "text": " Seoul",
      "label": "LOC"
    },
    {
      "text": " Republic of Korea",
      "label": "LOC"
    },
    {
      "text": "\nyesolpark",
      "label": "PER"
    },
    {
      "text": "hanyang",
      "label": "ORG"
    },
    {
      "text": "Department of Artificial Intelligence",
      "label": "ORG"
    },
    {
      "text": " Hanyang University",
      "label": "ORG"
    },
    {
      "text": " Seoul",
      "label": "LOC"
    },
    {
      "text": " Republic of Korea",
      "label": "LOC"
    },
    {
      "text": "cn",
      "label": "ORG"
    },
    {
      "text": "hanyang",
      "label": "ORG"
    },
    {
      "text": "Department of Biomedical Informatics",
      "label": "ORG"
    },
    {
      "text": " Hanyang University",
      "label": "ORG"
    },
    {
      "text": " Seoul",
      "label": "LOC"
    },
    {
      "text": " Republic of Korea",
      "label": "LOC"
    },
    {
      "text": " minarho",
      "label": "PER"
    },
    {
      "text": "hanyang",
      "label": "ORG"
    },
    {
      "text": "BioNER",
      "label": "MISC"
    },
    {
      "text": " BioNER",
      "label": "MISC"
    }
  ],
  "summary": "This review explores advancements in biomedical named entity recognition (BioNER), focusing on flat and nested entity structures using pre-trained language models. It highlights state-of-the-art performance metrics, ongoing challenges like annotation inconsistencies, and the limited availability of diverse entity types, offering insights for future research in the field.",
  "embedding": [
    0.04760240763425827,
    0.0041260067373514175,
    -0.0036404957063496113,
    -0.01434593740850687,
    -0.0073808468878269196,
    0.05290831997990608,
    0.017929470166563988,
    0.04295315593481064,
    0.03153444454073906,
    -0.040911491960287094,
    -0.009014255367219448,
    -0.03151632845401764,
    0.04969077184796333,
    0.020579827949404716,
    -0.016553640365600586,
    -0.04663636535406113,
    0.03607715293765068,
    0.01840321533381939,
    0.060685791075229645,
    0.019454127177596092,
    -0.03990720584988594,
    0.0008853429462760687,
    0.002616719575598836,
    0.030586980283260345,
    0.03628503903746605,
    0.007607826963067055,
    0.021960359066724777,
    -0.014166783541440964,
    0.04888235777616501,
    -0.07346109300851822,
    0.012889879755675793,
    0.019271839410066605,
    -0.047857437282800674,
    0.04363517463207245,
    2.021238515226287e-06,
    -0.016802841797471046,
    -0.052437856793403625,
    0.03774749115109444,
    -0.007959164679050446,
    -0.02970729023218155,
    0.008697014302015305,
    -0.028812114149332047,
    -0.0029039778746664524,
    -0.0340922549366951,
    -0.004659723024815321,
    -0.006652729585766792,
    0.041028980165719986,
    0.04885087534785271,
    0.004970795940607786,
    0.04609498754143715,
    -0.01033689547330141,
    -0.03440923988819122,
    0.06705261021852493,
    0.0258023738861084,
    -0.02239503152668476,
    -0.025133179500699043,
    0.061307068914175034,
    -0.02266749180853367,
    0.03331325575709343,
    0.014770177192986012,
    0.0014604025054723024,
    0.029475852847099304,
    -0.039038680493831635,
    0.05557505413889885,
    0.0403805747628212,
    0.06120912358164787,
    -0.020021913573145866,
    -0.003467818256467581,
    0.011865544132888317,
    0.057802584022283554,
    -0.02162686362862587,
    0.002557298168540001,
    0.05682666227221489,
    -0.014582742936909199,
    -0.030023645609617233,
    -0.018814079463481903,
    0.013419225811958313,
    -0.025251980870962143,
    -0.0805819109082222,
    0.01042598020285368,
    0.029372259974479675,
    0.007677876856178045,
    0.015398292802274227,
    -0.01265018805861473,
    -0.006594676990061998,
    0.030831994488835335,
    -0.03223174065351486,
    -0.03306438773870468,
    0.01692381128668785,
    -0.02158411778509617,
    0.0009357506642118096,
    -0.031648267060518265,
    0.05335773900151253,
    0.025437330827116966,
    0.015056557953357697,
    0.009292010217905045,
    -0.007791259791702032,
    -0.05693407729268074,
    0.0404810756444931,
    -0.07746424525976181,
    0.02313845418393612,
    -0.02215282991528511,
    0.001314133987762034,
    0.02199178747832775,
    -0.05138370022177696,
    -0.02542353793978691,
    -0.022276313975453377,
    0.05987074598670006,
    0.014753004536032677,
    0.05705861374735832,
    -0.05816515535116196,
    -0.0037191102746874094,
    -0.021190039813518524,
    0.05165708437561989,
    0.013247370719909668,
    -0.012163707986474037,
    -0.08214759826660156,
    -0.05421598255634308,
    -0.00558907026425004,
    -0.013147266581654549,
    0.018108010292053223,
    -0.0006241221562959254,
    -0.011158104054629803,
    0.034470438957214355,
    -0.028963973745703697,
    0.010880881920456886,
    0.007797146216034889,
    -0.02108924463391304,
    -0.04014371708035469,
    -0.03404509276151657,
    0.019547216594219208,
    -0.007776038721203804,
    0.04955260828137398,
    -0.023550095036625862,
    0.011219091713428497,
    0.058423083275556564,
    0.03323032334446907,
    -0.07162211835384369,
    0.0009407727047801018,
    -0.027737807482481003,
    -0.01937209442257881,
    -0.02186162769794464,
    0.0038485988043248653,
    -0.008029602468013763,
    0.04547448083758354,
    0.009095861576497555,
    -0.012124762870371342,
    -0.03658780828118324,
    0.03321131318807602,
    -0.00958006177097559,
    -0.06673535704612732,
    0.07369963079690933,
    -0.07025527954101562,
    -0.04176253825426102,
    0.04310154169797897,
    0.012245007790625095,
    0.04376547783613205,
    -0.028385814279317856,
    0.05088227242231369,
    -0.01815626211464405,
    0.019301971420645714,
    -0.013173423707485199,
    0.008662342093884945,
    -0.023068124428391457,
    0.023887131363153458,
    -0.11804911494255066,
    0.03891734033823013,
    -0.008432859554886818,
    -0.045015547424554825,
    0.0016590921441093087,
    -0.01875348761677742,
    0.0035623435396701097,
    0.03165138140320778,
    0.019138308241963387,
    0.010801344178617,
    0.0880526676774025,
    0.025510551407933235,
    -0.029323766008019447,
    -0.00952108297497034,
    -0.037430983036756516,
    0.009907463565468788,
    -0.00293629034422338,
    0.008502052165567875,
    0.0004417326708789915,
    -0.02854558266699314,
    0.002547614509239793,
    -0.01773744262754917,
    0.03480936959385872,
    0.025412673130631447,
    -0.03335132077336311,
    -0.019269142299890518,
    0.012081651948392391,
    -0.019418027251958847,
    -0.05369884893298149,
    0.044621989130973816,
    -0.02845059335231781,
    -0.06478288024663925,
    0.038635075092315674,
    0.04398119077086449,
    -0.023745834827423096,
    0.02644726075232029,
    -0.034574296325445175,
    -0.02785755880177021,
    0.0020292496774345636,
    0.03129158541560173,
    -0.07504395395517349,
    -0.06418710201978683,
    0.003688092576339841,
    -0.08836735785007477,
    -0.004701282363384962,
    -0.038860708475112915,
    -0.019181836396455765,
    0.009215909987688065,
    -0.032343290746212006,
    -0.018344033509492874,
    0.06803415715694427,
    0.01625761017203331,
    0.010604199953377247,
    -0.035295519977808,
    0.07076738029718399,
    -0.0076385424472391605,
    0.0018738409271463752,
    -0.006805198732763529,
    0.03472837060689926,
    -0.02752205729484558,
    -0.029078809544444084,
    0.004925271961838007,
    0.027717236429452896,
    0.048924367874860764,
    0.03603188693523407,
    0.0017603534506633878,
    0.010090209543704987,
    0.03408902883529663,
    -0.02825741283595562,
    -0.0414140559732914,
    -0.03618569299578667,
    0.04055701196193695,
    -0.04170607775449753,
    -0.037195052951574326,
    0.02542140521109104,
    -0.017726529389619827,
    -0.04122009500861168,
    -0.021189721301198006,
    -0.015096422284841537,
    -0.04702097550034523,
    0.013698045164346695,
    0.011438467539846897,
    -0.008726763539016247,
    0.04443204030394554,
    0.024686412885785103,
    0.02646561712026596,
    -0.03305049240589142,
    0.061400774866342545,
    -0.01867772452533245,
    -0.013930873945355415,
    -0.020891712978482246,
    -0.12075330317020416,
    0.006529584992676973,
    -0.06808892637491226,
    0.02473892644047737,
    0.01246350072324276,
    -0.009732170030474663,
    -0.04405069723725319,
    -0.0021783672273159027,
    -0.00010876862506847829,
    -0.03496110439300537,
    -0.018675832077860832,
    -0.02653704583644867,
    -0.005536770448088646,
    0.03851066902279854,
    0.055596768856048584,
    -0.023742277175188065,
    0.011901202611625195,
    0.0015038864221423864,
    -0.03718618303537369,
    -0.018947254866361618,
    0.022000504657626152,
    -0.02796242944896221,
    -0.030545014888048172,
    0.062024980783462524,
    0.03821440786123276,
    -0.022979358211159706,
    0.02050022780895233,
    0.022237593308091164,
    -0.08657164126634598,
    0.03332633525133133,
    0.01441259402781725,
    -0.09690812975168228,
    -0.013585886918008327,
    0.011034917086362839,
    -0.03708186373114586,
    -0.0035794535651803017,
    0.017129581421613693,
    -0.004408523440361023,
    -0.02762477844953537,
    0.03679647296667099,
    0.0021091317757964134,
    0.07769037783145905,
    0.007689226884394884,
    0.035515088587999344,
    0.009543420746922493,
    0.009597745724022388,
    0.03575661778450012,
    0.03225286304950714,
    -0.04699268564581871,
    0.06444046646356583,
    0.018750347197055817,
    -0.049412913620471954,
    0.05644526705145836,
    0.04393007978796959,
    0.012145780958235264,
    0.028011195361614227,
    -0.016939718276262283,
    0.004474899731576443,
    -0.023474305868148804,
    0.0066648502834141254,
    -0.011924652382731438,
    0.02732354961335659,
    -0.03344030678272247,
    0.02885127253830433,
    0.012325710617005825,
    -0.00808672234416008,
    0.07876182347536087,
    -0.003515574149787426,
    0.01649973914027214,
    -0.049164704978466034,
    0.01435130275785923,
    -0.022504620254039764,
    -0.014973864890635014,
    0.028338463976979256,
    0.0326681062579155,
    0.017513848841190338,
    -0.07016535103321075,
    0.06202400103211403,
    0.03246978297829628,
    -0.01750093139708042,
    -0.048001810908317566,
    -0.013151941820979118,
    0.0005789589486084878,
    0.005021891091018915,
    -0.014990976080298424,
    -0.02813006192445755,
    -0.008390698581933975,
    0.037381600588560104,
    -0.03404945135116577,
    0.004751982633024454,
    -0.012975539080798626,
    -0.015973201021552086,
    -0.001197482692077756,
    0.06386344879865646,
    0.003383260453119874,
    0.012727092020213604,
    -0.0009114984422922134,
    0.010172257199883461,
    -0.03787970542907715,
    0.06597621738910675,
    0.025804797187447548,
    -0.036878813058137894,
    0.0059844194911420345,
    0.001717733801342547,
    -0.0055591510608792305,
    0.07113271206617355,
    -0.056217871606349945,
    0.012244444340467453,
    0.026385996490716934,
    -0.022486630827188492,
    0.008074194192886353,
    -0.051812268793582916,
    0.013209180906414986,
    -0.026546034961938858,
    0.03743743151426315,
    -0.03305148333311081,
    0.03397590294480324,
    0.07617202401161194,
    0.025183431804180145,
    -0.033882927149534225,
    0.000932307739276439,
    -0.0004906195681542158,
    -0.048726245760917664,
    -0.04604219272732735,
    0.024464331567287445,
    -0.07381775230169296,
    -0.0019008866511285305,
    0.02045402303338051,
    -0.02014968730509281,
    -0.05346905440092087,
    -0.08954222500324249,
    0.014776503667235374,
    -0.032179512083530426,
    0.04939236119389534,
    -0.018872229382395744,
    -0.05665482208132744,
    -0.03265371173620224,
    -0.05132360756397247,
    -0.0013314413372427225,
    -0.010743342339992523,
    -0.028659893199801445,
    -0.021641407161951065,
    0.01364205963909626,
    -0.02528219297528267,
    0.01814938150346279,
    -0.05106358602643013,
    -0.059088971465826035,
    0.03856813535094261,
    -0.023780591785907745,
    -0.009218066930770874,
    0.007855101488530636,
    0.069046750664711,
    -0.009340262971818447,
    0.006889330688863993,
    0.013963946141302586,
    -0.009510301984846592,
    -0.009889583103358746,
    -0.05190189182758331,
    0.04790706932544708,
    0.014384015463292599,
    -0.012327163480222225,
    -0.051727570593357086,
    0.015987040475010872,
    -0.021702725440263748,
    -0.026044243946671486,
    -0.0039976779371500015,
    0.08982241898775101,
    0.014613360166549683,
    -0.048380088061094284,
    -0.04770078882575035,
    0.007586885709315538,
    0.015069350600242615,
    0.03555067256093025,
    -0.010379800572991371,
    -0.017876382917165756,
    -0.03874975070357323,
    0.03508268669247627,
    0.07863613963127136,
    -0.014598218724131584,
    -0.033514440059661865,
    0.050006818026304245,
    -0.08198944479227066,
    -0.033415306359529495,
    -0.02949751541018486,
    0.001372811384499073,
    0.000746010453440249,
    0.04734316095709801,
    0.03240284323692322,
    0.045997437089681625,
    0.0947609394788742,
    0.01007938850671053,
    -0.03397125005722046,
    0.008943043649196625,
    0.03104424849152565,
    0.008198079653084278,
    -0.0648026242852211,
    -0.06768611073493958,
    0.03302927315235138,
    0.026451991870999336,
    -0.04130379110574722,
    -0.00731059443205595,
    0.008928648196160793,
    -0.026236439123749733,
    -0.030582226812839508,
    0.0184551402926445,
    0.008972769603133202,
    -0.041981082409620285,
    -0.00679752416908741,
    -0.017806051298975945,
    0.10361533612012863,
    0.021706728264689445,
    -0.03287043794989586,
    -0.016861651092767715,
    -0.0320245735347271,
    -0.005758555140346289,
    -0.062173545360565186,
    0.010407799854874611,
    -0.004950736649334431,
    0.026793397963047028,
    -0.003906578291207552,
    0.015932533890008926,
    -0.024426134303212166,
    -0.006616909988224506,
    0.03438815474510193,
    0.035798631608486176,
    0.07256503403186798,
    -0.012800655327737331,
    -0.044467512518167496,
    -0.02455834671854973,
    0.02615177258849144,
    0.002350709168240428,
    0.02570091001689434,
    0.017774268984794617,
    -0.06051969900727272,
    -0.0015755209606140852,
    0.020264532417058945,
    -0.05614437162876129,
    -0.004155307542532682,
    -0.031175712123513222,
    -0.06328947842121124,
    -0.01752506196498871,
    0.027375834062695503,
    0.05076999217271805,
    0.041018784046173096,
    0.03425661846995354,
    -0.029302192851901054,
    0.01229143887758255,
    0.013168207369744778,
    0.024255983531475067,
    0.036569878458976746,
    -0.034095458686351776,
    0.027158483862876892,
    0.007305262144654989,
    0.017698481678962708,
    -0.0705885961651802,
    0.025561021640896797,
    -0.015694979578256607,
    0.0721561461687088,
    -0.018822183832526207,
    0.014324757270514965,
    0.011623736470937729,
    0.00023565937590319663,
    0.02504708804190159,
    0.017490552738308907,
    -0.01856764778494835,
    0.0019194527994841337,
    0.038927365094423294,
    -0.053507767617702484,
    0.002325039356946945,
    0.03913234919309616,
    -0.003012473462149501,
    -0.036386966705322266,
    -0.020272960886359215,
    0.03492323309183121,
    0.023882417008280754,
    0.04376375675201416,
    0.0342680998146534,
    -0.097980797290802,
    -0.029288582503795624,
    0.005732214543968439,
    0.053843770176172256,
    0.004556371830403805,
    -0.04286101460456848,
    0.010702795349061489,
    -0.05232878401875496,
    -0.02619851939380169,
    0.07051926851272583,
    -0.006226073019206524,
    0.026856746524572372,
    0.026036841794848442,
    -0.03435182571411133,
    -0.024943269789218903,
    0.02474827878177166,
    -0.10156124085187912,
    -0.027741970494389534,
    0.03690902516245842,
    -0.02861662395298481,
    -0.039602458477020264,
    0.025332169607281685,
    -5.978981568776696e-33,
    -0.005851799622178078,
    -0.06282827258110046,
    -0.002111702458932996,
    0.03354320675134659,
    -0.012447435408830643,
    0.04872217774391174,
    -0.033417899161577225,
    -0.013115175068378448,
    0.028026442974805832,
    0.018057329580187798,
    0.002589600393548608,
    -0.0509602390229702,
    0.03362355753779411,
    -0.02881336770951748,
    0.0046691601164639,
    -0.011390934698283672,
    0.0004051049763802439,
    0.03325628116726875,
    0.011555580422282219,
    -0.005031498149037361,
    0.052821703255176544,
    0.062203921377658844,
    0.06866974383592606,
    -0.09126923978328705,
    0.008888032287359238,
    0.04731883108615875,
    0.026652351021766663,
    -0.003463095985352993,
    0.012841888703405857,
    0.052435848861932755,
    -0.0532546192407608,
    -0.027872227132320404,
    0.02166895568370819,
    0.05350591242313385,
    0.011515568941831589,
    -0.025501059368252754,
    -0.11369390785694122,
    0.035009145736694336,
    -0.05499726161360741,
    0.06105111911892891,
    -0.030047092586755753,
    -0.0322309210896492,
    0.03858320787549019,
    -0.01007700152695179,
    -0.045754313468933105,
    -0.019093411043286324,
    0.05189911276102066,
    -0.014219291508197784,
    -0.006698152516037226,
    0.002866635099053383,
    -0.09523157775402069,
    0.055734336376190186,
    0.028645960614085197,
    0.05240718647837639,
    0.052919916808605194,
    -0.04522833228111267,
    0.01633213460445404,
    0.04683970659971237,
    -0.028437308967113495,
    -0.010701037012040615,
    0.051871009171009064,
    -0.0007926073158159852,
    0.012196398340165615,
    0.00845323596149683,
    -0.007501539774239063,
    0.03391581028699875,
    0.03242669627070427,
    -0.01592865213751793,
    -0.03158425912261009,
    -0.005401181057095528,
    -0.020201312378048897,
    0.03284864500164986,
    -0.021237293258309364,
    0.019814200699329376,
    0.0758042186498642,
    -0.019357269629836082,
    -0.09127964824438095,
    0.04412313923239708,
    -0.02393243834376335,
    0.004134771879762411,
    -0.03750794380903244,
    -0.02603057585656643,
    -0.042929161339998245,
    -0.012732427567243576,
    0.028094058856368065,
    -0.03569532558321953,
    -0.029781578108668327,
    -0.013086006045341492,
    -0.008229243569076061,
    -0.025564834475517273,
    -0.03829493373632431,
    0.05593883618712425,
    -0.0007304364116862416,
    -0.025994570925831795,
    -0.029797064140439034,
    -0.03625573217868805,
    0.07724800705909729,
    0.02877471223473549,
    -0.04391933232545853,
    -0.00309145194478333,
    -0.03421735018491745,
    -0.06837967783212662,
    0.01490488275885582,
    -0.010866322554647923,
    0.0463993214070797,
    -0.0072179571725428104,
    0.030799008905887604,
    -0.021694637835025787,
    -0.03846059739589691,
    -0.011844243854284286,
    0.03308646008372307,
    -0.022054724395275116,
    0.029684852808713913,
    0.06345951557159424,
    0.005203663371503353,
    -0.007782930042594671,
    0.017248574644327164,
    0.018822360783815384,
    0.06024586409330368,
    0.012590955942869186,
    -0.004572492558509111,
    0.03304244950413704,
    0.03224243223667145,
    -0.00017278421728406101,
    -0.015018288046121597,
    -0.017841385677456856,
    -0.025179283693432808,
    0.03589670732617378,
    -0.011508999392390251,
    -0.06824309378862381,
    -0.002603869652375579,
    -0.024492569267749786,
    2.744490927852894e-07,
    0.04569416493177414,
    0.03755443915724754,
    0.03087945468723774,
    -0.05706406384706497,
    -0.011039266362786293,
    -0.012536795809864998,
    -0.0011447484139353037,
    0.03703322634100914,
    -0.07053496688604355,
    0.017382485792040825,
    0.04702568054199219,
    -0.05016666650772095,
    -0.009588029235601425,
    0.02405739761888981,
    -0.0443589873611927,
    -0.04882686957716942,
    -0.04213086888194084,
    -0.012644477188587189,
    -0.03289378061890602,
    0.015675103291869164,
    0.0025918898172676563,
    0.03213188052177429,
    -0.009321235120296478,
    -0.013756129890680313,
    0.02096252702176571,
    0.019527988508343697,
    0.01458499301224947,
    0.007588712032884359,
    0.04222826287150383,
    0.04806013032793999,
    -0.0497639924287796,
    -0.01901833340525627,
    0.011651480570435524,
    0.05332630127668381,
    -0.004560060333460569,
    0.013226793147623539,
    0.03843121975660324,
    0.07895275950431824,
    -0.0455322302877903,
    0.057068441063165665,
    0.03245798870921135,
    0.026002418249845505,
    0.04273577779531479,
    -0.009536095894873142,
    0.03374917060136795,
    0.013557685539126396,
    -0.05382058396935463,
    0.006160138640552759,
    0.01403713971376419,
    0.00750806275755167,
    0.012858972884714603,
    0.01155473105609417,
    -0.007763119414448738,
    0.06208549067378044,
    0.03171860799193382,
    0.008711075410246849,
    -0.004770661238580942,
    -0.06334778666496277,
    0.04868113622069359,
    0.0024716523475944996,
    -0.015167554840445518,
    0.01094663143157959,
    0.036059167236089706,
    0.02069154754281044,
    0.05427519232034683,
    -0.07273326069116592,
    0.00019830289238598198,
    2.998187651792258e-34,
    -0.0002214007399743423,
    -0.021259494125843048,
    -0.04602259770035744,
    0.026221921667456627,
    0.013499859720468521,
    -0.021422578021883965,
    -0.009653495624661446,
    0.01391140278428793,
    -0.04068771377205849,
    -0.05185692012310028,
    -0.004534987732768059
  ]
}